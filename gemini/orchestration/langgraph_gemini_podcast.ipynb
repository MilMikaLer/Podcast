{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MilMikaLer/Podcast/blob/main/gemini/orchestration/langgraph_gemini_podcast.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Build Your Own AI Podcasting Agent with LangGraph, Gemini, and Chirp 3\n",
        "## AI-Powered Podcast Creation with Automated Research, Writing, and Refinement\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/orchestration/langgraph_gemini_podcast.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Forchestration%2Flanggraph_gemini_podcast.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/orchestration/langgraph_gemini_podcast.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/orchestration/langgraph_gemini_podcast.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>\n",
        "\n",
        "<b>Share to:</b>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/orchestration/langgraph_gemini_podcast.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/orchestration/langgraph_gemini_podcast.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/orchestration/langgraph_gemini_podcast.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/orchestration/langgraph_gemini_podcast.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/orchestration/langgraph_gemini_podcast.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
        "</a>            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84f0f73a0f76"
      },
      "source": [
        "| Author |\n",
        "| --- |\n",
        "| [Kristopher Overholt](https://github.com/koverholt/) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Creating a podcast can be a very involved process, requiring extensive research, writing, editing, and production. **What if there was a way to leverage the power of AI to streamline the creation of a podcast, automating many of the tasks traditionally performed by humans?** [NotebookLM](https://notebooklm.google.com/), for example, lets users easily generate [audio overviews based on documents](https://blog.google/technology/ai/notebooklm-audio-overviews/).\n",
        "\n",
        "#### 🔈🔈 [Listen to a sample podcast generated by this notebook!](https://storage.googleapis.com/github-repo/generative-ai/gemini/orchestration/langgraph/gemini-podcast.mp3) 🔈🔈\n",
        "\n",
        "But what if you want to customize the length of the podcast, the voices, or the conversation flow and augment it with additional research tools? In this notebook, **you'll recreate this kind of podcast generation functionality by building an AI agent to do the heavy lifting and then customize the entire flow yourself!**\n",
        "\n",
        "**This notebook demonstrates how to build a [LangGraph](https://langchain-ai.github.io/langgraph/)-powered AI agent to research, write, and refine a podcast script using the [Gemini API in in Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models).** You'll use LangGraph and LangChain to orchestrate calls to Gemini along with calls to different search tools, allowing the AI to learn about a given topic before writing about it. Then, the AI will critique its work and iterate on the podcast script, improving it with each revision.\n",
        "\n",
        "Here's how you'll build and use our AI podcasting agent:\n",
        "\n",
        "- **[User]** Define the podcast topic: Provide a clear and concise topic for the podcast.\n",
        "- **[Agent]** Generate an outline: Use Gemini to create a high-level outline, structuring the podcast's flow.\n",
        "- **[Agent]** Conduct research: The AI agent will use search tools like arXiv, PubMed, and Wikipedia to gather relevant information.\n",
        "- **[Agent]** Write a script: Gemini will generate an engaging podcast script, incorporating the research findings.\n",
        "- **[Agent]** Critique and iterate: The agent will analyze its script, provide a critique, then generate a revised draft.\n",
        "- **[Agent]** Generate audio: You'll use text-to-speech to generate audio for each line of the podcast script.\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/github-repo/generative-ai/gemini/orchestration/langgraph/gemini-podcast-agent.jpg\" width=\"400px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61RBz8LLbxCR"
      },
      "source": [
        "## Get started\n",
        "\n",
        "This section sets up the environment for the AI podcast agent. This includes:\n",
        "\n",
        "- **Installing Libraries:**  Installing the required Python libraries\n",
        "- **Restarting Runtime (Colab Only):**  Restarting the Colab runtime\n",
        "- **Authenticating Environment (Colab Only):**  Authenticating to Google Cloud\n",
        "- **Setting Project Information:**  Setting up your Google Cloud project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Install Vertex AI SDK and other required packages\n",
        "\n",
        "This code cell installs the necessary Python libraries for running the AI podcast agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tFy3H3aPgx12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c205ad87-ac80-492d-f13c-7fb1fa0777a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.1/188.1 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.7/147.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.1/434.1 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "%pip install -q -U \\\n",
        "    arxiv \\\n",
        "    google-cloud-aiplatform \\\n",
        "    google-cloud-texttospeech \\\n",
        "    langgraph \\\n",
        "    langchain-google-vertexai \\\n",
        "    langchain-community \\\n",
        "    pydub \\\n",
        "    pymupdf \\\n",
        "    wikipedia \\\n",
        "    xmltodict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWOrTJ3gx13"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you're running this notebook on Google Colab, run the cell below to authenticate your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NyKGtVQjgx13"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF4l8DTdWgPY"
      },
      "source": [
        "### Set Google Cloud project information and initialize Vertex AI SDK\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Nqwi-5ufWp_B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import vertexai\n",
        "\n",
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\", isTemplate: true}\n",
        "if PROJECT_ID == \"[your-project-id]\":\n",
        "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "\n",
        "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdvJRUWRNGHE"
      },
      "source": [
        "## Building the AI podcasting agent\n",
        "\n",
        "This section constructs the AI agent. Key steps include:\n",
        "\n",
        "- **Initializing Agent Memory and State:** Setting up the agent's memory and defining its data structure\n",
        "- **Initializing the Gemini Model:**  Loading the Gemini language model from Vertex AI\n",
        "- **Defining Search Tools:**  Creating tools to access information sources like arXiv, PubMed, and Wikipedia\n",
        "- **Defining Workflow Stages:** Defining each stage of the workflow, including prompts and functions\n",
        "- **Compiling the Workflow:**  Structuring the workflow as a graph using LangGraph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5303c05f7aa6"
      },
      "source": [
        "### Import libraries\n",
        "\n",
        "This section imports the necessary libraries for LangGraph, LangChain, Vertex AI, and other utilities needed for your agent's functionality.\n",
        "\n",
        "This includes tools for interacting with the Gemini API, defining custom tools, managing agent state, and displaying results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6fc324893334"
      },
      "outputs": [],
      "source": [
        "# Common libraries\n",
        "import logging\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Typing utilities for data validation and schema definitions\n",
        "from typing import TypedDict\n",
        "\n",
        "from IPython.display import Audio, Image\n",
        "\n",
        "# Libraries for text-to-speech generation and audio processing\n",
        "from google.cloud import texttospeech\n",
        "from langchain.schema.document import Document\n",
        "\n",
        "# Tools\n",
        "from langchain_community.retrievers import (\n",
        "    ArxivRetriever,\n",
        "    PubMedRetriever,\n",
        "    WikipediaRetriever,\n",
        ")\n",
        "\n",
        "# LangChain and LangGraph components for message handling and tool integration\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, ToolMessage\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "# LangChain integrations for Gemini API in Google AI Studio and Vertex AI\n",
        "from langchain_google_vertexai import ChatVertexAI\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import END, StateGraph\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# Set logging level to ERROR to filter warnings\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.ERROR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e43229f3ad4f"
      },
      "source": [
        "### Initialize agent memory and agent state\n",
        "\n",
        "Here, you initialize your [agent's memory](https://langchain-ai.github.io/langgraph/how-tos/memory/manage-conversation-history/) to store information during the workflow.\n",
        "\n",
        "You also define the schema for your [agent's state](https://langchain-ai.github.io/langgraph/how-tos/state-model/), which includes the podcast outline, search queries, and drafts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "cf93d5f0ce00"
      },
      "outputs": [],
      "source": [
        "# Initialize agent memory\n",
        "memory = MemorySaver()\n",
        "\n",
        "\n",
        "# Define the agent's state\n",
        "class AgentState(TypedDict):\n",
        "    revision_number: int\n",
        "    max_revisions: int\n",
        "    search_count: int\n",
        "    max_searches: int\n",
        "    task: str\n",
        "    outline: str\n",
        "    queries: list\n",
        "    content: list\n",
        "    draft: str\n",
        "    critique: str\n",
        "    tool_calls: list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27b61a7e7ef6"
      },
      "source": [
        "### Initialize Gemini model\n",
        "\n",
        "Initialize the Gemini model from Vertex AI, specifying the model version and temperature settings.\n",
        "\n",
        "This sets up the core language model that will power your agent's actions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "06877aae6673"
      },
      "outputs": [],
      "source": [
        "model = ChatVertexAI(model=\"gemini-2.0-flash\", temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d591fae74758"
      },
      "source": [
        "### Define search tools\n",
        "\n",
        "This section defines custom tools that will be used by your AI podcast agent to gather information from various sources. These tools act as interfaces to external services and provide access to relevant data for the podcast topic.\n",
        "\n",
        "Each tool is implemented as a Python function decorated with the `@tool` decorator from LangChain. This decorator makes it easy to integrate these functions into LangGraph workflows.\n",
        "\n",
        "The following search tools are defined:\n",
        "\n",
        "- **`search_arxiv`:** Retrieves research papers from arXiv based on a keyword query.\n",
        "- **`search_pubmed`:** Searches for information on PubMed, a database of biomedical literature.\n",
        "- **`search_wikipedia`:** Fetches information from Wikipedia based on a keyword query.\n",
        "\n",
        "Your LangGraph application will use these tool nodes to call the corresponding search functions and obtain information from these external sources. This allows the AI agent to learn about the podcast topic before generating the script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0d27ed8a91c1"
      },
      "outputs": [],
      "source": [
        "@tool\n",
        "def search_arxiv(query: str) -> list[Document]:\n",
        "    \"\"\"Search for relevant publications on arXiv\"\"\"\n",
        "    retriever = ArxivRetriever(\n",
        "        load_max_docs=2,\n",
        "        get_full_documents=True,\n",
        "    )\n",
        "    docs = retriever.invoke(query)\n",
        "    if docs:\n",
        "        return docs\n",
        "    else:\n",
        "        return [\"No results found on arXiv\"]\n",
        "\n",
        "\n",
        "@tool\n",
        "def search_pubmed(query: str) -> list[Document]:\n",
        "    \"\"\"Search for information on PubMed\"\"\"\n",
        "    retriever = PubMedRetriever()\n",
        "    docs = retriever.invoke(query)\n",
        "    if docs:\n",
        "        return docs\n",
        "    else:\n",
        "        return [\"No results found on PubMed\"]\n",
        "\n",
        "\n",
        "@tool\n",
        "def search_wikipedia(query: str) -> list[Document]:\n",
        "    \"\"\"Search for information on Wikipedia\"\"\"\n",
        "    retriever = WikipediaRetriever()\n",
        "    docs = retriever.invoke(query)\n",
        "    if docs:\n",
        "        return docs\n",
        "    else:\n",
        "        return [\"No results found on Wikipedia\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95fbea6aaa20"
      },
      "source": [
        "### Define workflow stages along with corresponding prompts and functions\n",
        "\n",
        "This section defines the different stages of the AI podcast agent's workflow and the corresponding prompt templates and node functions that drive each stage.\n",
        "\n",
        "Each stage represents a specific task in the podcast creation process, such as generating an outline, conducting research, writing the script, and providing a critique.\n",
        "\n",
        "For each stage, you'll define:\n",
        "\n",
        "- **Prompt Template:** A carefully crafted text prompt that instructs the Gemini language model on what to do at that stage. The prompt provides context, instructions, and any necessary input data.\n",
        "- **Node Function:** A Python function that encapsulates the logic for executing that stage. The function typically involves:\n",
        "    - Constructing the prompt with relevant information from the agent's state.\n",
        "    - Invoking the Gemini API with the prompt.\n",
        "    - Processing the model's response and updating the agent's state.\n",
        "\n",
        "These prompt templates and node functions are the building blocks of the LangGraph workflow that orchestrates the entire podcast creation process.\n",
        "\n",
        "#### Podcast outline node\n",
        "\n",
        "This node generates a structured outline for the podcast based on the user-provided topic:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4ce60bbc06e6"
      },
      "outputs": [],
      "source": [
        "OUTLINE_PROMPT = \"\"\"You are an expert writer tasked with writing a high level outline of an engaging 2-minute podcast.\n",
        "Write such an outline for the user provided topic. Give an outline of the podcast along with any\n",
        "relevant notes or instructions for the sections.\"\"\"\n",
        "\n",
        "\n",
        "# Generate an outline for the podcast based on the user-provided topic\n",
        "def podcast_outline_node(state: AgentState):\n",
        "    messages = [\n",
        "        SystemMessage(content=OUTLINE_PROMPT),\n",
        "        HumanMessage(content=state[\"task\"]),\n",
        "    ]\n",
        "    response = model.invoke(messages)\n",
        "    return {\"outline\": response.content}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d7349c32d28"
      },
      "source": [
        "#### Research plan node\n",
        "\n",
        "This node formulates a search query based on the podcast topic and previous queries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87df19f53b95"
      },
      "outputs": [],
      "source": [
        "RESEARCH_PLAN_PROMPT = \"\"\"You are a researcher tasked with providing information that can\n",
        "be used when writing the following podcast. Generate one search query consisting of a few\n",
        "keywords that will be used to gather any relevant information. Do not output any information\n",
        "other than the query consisting of a few words.\n",
        "\n",
        "These were the past queries, do not repeat keywords from past queries in your newly generated query:\n",
        "---\n",
        "{queries}\"\"\"\n",
        "\n",
        "\n",
        "# Generates a search query based on the outline\n",
        "def research_plan_node(state: AgentState):\n",
        "    messages = [\n",
        "        SystemMessage(content=RESEARCH_PLAN_PROMPT.format(queries=state[\"queries\"])),\n",
        "        HumanMessage(content=state[\"task\"]),\n",
        "    ]\n",
        "    response = model.invoke(messages)\n",
        "    queries = state[\"queries\"]\n",
        "    if queries:\n",
        "        queries.append(response.content)\n",
        "    else:\n",
        "        queries = [response.content]\n",
        "    return {\"queries\": queries}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1719cf80233f"
      },
      "source": [
        "#### Research task node\n",
        "\n",
        "This node executes a search using the selected tool and query, retrieving relevant information for the podcast:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "9bcbfe53b7d9"
      },
      "outputs": [],
      "source": [
        "RESEARCH_TASK_PROMPT = \"\"\"Use the available search tools and search queries to find information\n",
        "relevant to the podcast. Try searching different sources to obtain different articles. Try using\n",
        "different search tools than what was used previously so that you can obtain a broader range of\n",
        "information.\n",
        "\n",
        "These are the previous tool calls, so you can choose a different tool:\n",
        "---\n",
        "{tool_calls}\n",
        "---\n",
        "These are the previous search results, so you can aim for different sources and content:\n",
        "---\n",
        "{content}\"\"\"\n",
        "\n",
        "\n",
        "# Performs searches using tools\n",
        "def research_agent_node(state: AgentState):\n",
        "    tool_calls = state[\"tool_calls\"]\n",
        "    content = state[\"content\"]\n",
        "    queries = state[\"queries\"]\n",
        "    query = queries[-1]\n",
        "    messages = [\n",
        "        SystemMessage(\n",
        "            content=RESEARCH_TASK_PROMPT.format(tool_calls=tool_calls, content=content)\n",
        "        ),\n",
        "        HumanMessage(content=query),\n",
        "    ]\n",
        "\n",
        "    # Perform function calls\n",
        "    tools = [search_arxiv, search_pubmed, search_wikipedia]\n",
        "    model_with_tools = model.bind_tools(tools)\n",
        "    response_tool_calls = model_with_tools.invoke(messages)\n",
        "    if tool_calls:\n",
        "        tool_calls.append(response_tool_calls)\n",
        "    else:\n",
        "        tool_calls = [response_tool_calls]\n",
        "\n",
        "    # Defines a tool node based on search functions\n",
        "    tool_node = ToolNode(tools)\n",
        "    response = tool_node.invoke({\"messages\": [response_tool_calls]})\n",
        "\n",
        "    # Add the search results to the content list in the agent state\n",
        "    for message in response.get(\"messages\", []):\n",
        "        if isinstance(message, ToolMessage):\n",
        "            content.insert(0, message.content)\n",
        "\n",
        "    return {\n",
        "        \"content\": content,\n",
        "        \"tool_calls\": tool_calls,\n",
        "        \"search_count\": state[\"search_count\"] + 1,\n",
        "    }\n",
        "\n",
        "\n",
        "# Determine whether to continue research based on the number of searches performed\n",
        "def should_continue_tools(state: AgentState):\n",
        "    if state[\"search_count\"] > state[\"max_searches\"]:\n",
        "        return \"generate_script\"\n",
        "    else:\n",
        "        return \"research_plan\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eaa6d0bff8d"
      },
      "source": [
        "#### Podcast writing node\n",
        "\n",
        "This node generates a draft podcast script using the outline and research results, aiming for an engaging and informative style:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "1742523735e8"
      },
      "outputs": [],
      "source": [
        "WRITER_PROMPT = \"\"\"\n",
        "You are a writing assistant tasked with writing engaging 2-minute podcast scripts.\n",
        "\n",
        "- Generate the best podcast script possible for the user's request and the initial outline.\n",
        "- The script MUST strictly alternate lines between the two hosts, separating each host's line with a newline.\n",
        "- Add an intro phrase and outro phrase to start and end the podcast, and use a fun, random name for the podcast show.\n",
        "- Given a critique, respond with a revised version of your previous script.\n",
        "- Include lively back-and-forth chatter, reflections, and expressions of amazement between the hosts.\n",
        "- Cite at least THREE pieces of research throughout the script, choosing the most relevant research for each point.\n",
        "- DO NOT include ANY of the following:\n",
        "    - Speaker labels (e.g., \"Host 1:\", \"Host 2:\")\n",
        "    - Sound effect descriptions (e.g., \"[Sound of waves]\")\n",
        "    - Formatting instructions (e.g., \"(Emphasis)\", \"[Music fades in]\")\n",
        "    - Any other non-dialogue text.\n",
        "- Use this format for citations, including the month and year if available:\n",
        "    \"In [Month, Year], [Organization] found that...\"\n",
        "    \"Research from [Organization] in [Month, Year] showed that...\"\n",
        "    \"Back in [Month, Year], a study by [Organization] suggested that...\"\n",
        "---\n",
        "Utilize all of the following search results and context as needed:\n",
        "{content}\n",
        "---\n",
        "If this is a revision, the critique will be provided below:\n",
        "{critique}\"\"\"\n",
        "\n",
        "\n",
        "# Generates a draft of the script based on the content and outline\n",
        "def generate_script_node(state: AgentState):\n",
        "    messages = [\n",
        "        SystemMessage(\n",
        "            content=WRITER_PROMPT.format(\n",
        "                content=state[\"content\"], critique=state.get(\"critique\", \"\")\n",
        "            )\n",
        "        ),\n",
        "        HumanMessage(\n",
        "            content=f\"{state['task']}\\n\\nHere is my outline:\\n\\n{state['outline']}\"\n",
        "        ),\n",
        "    ]\n",
        "    response = model.invoke(messages)\n",
        "    return {\n",
        "        \"draft\": response.content,\n",
        "        \"search_count\": 0,  # Reset the search count for the next revision\n",
        "        \"revision_number\": state.get(\"revision_number\", 1) + 1,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d00163e72e16"
      },
      "source": [
        "#### Podcast critique node\n",
        "\n",
        "This node provides feedback and suggestions for improvement on the generated podcast script:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "de70a68caa8d"
      },
      "outputs": [],
      "source": [
        "CRITIQUE_PROMPT = \"\"\"You are a producer grading a podcast script.\n",
        "Generate critique and recommendations for the user's submission.\n",
        "Provide detailed recommendations, including requests for conciseness, depth, style, etc.\"\"\"\n",
        "\n",
        "\n",
        "# Generates a critique with feedback on the draft podcast script\n",
        "def perform_critique_node(state: AgentState):\n",
        "    messages = [\n",
        "        SystemMessage(content=CRITIQUE_PROMPT),\n",
        "        HumanMessage(content=state[\"draft\"]),\n",
        "    ]\n",
        "    response = model.invoke(messages)\n",
        "    return {\"critique\": response.content}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46aa739f9c1e"
      },
      "source": [
        "#### Research critique node\n",
        "\n",
        "This node generates a new search query based on the critique of the script, aiming to address weaknesses and find additional information:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "48682bcbb177"
      },
      "outputs": [],
      "source": [
        "RESEARCH_CRITIQUE_PROMPT = \"\"\"You are a writing assistant tasked with providing information that can\n",
        "be used when making any requested revisions (as outlined below).\n",
        "Generate one search query consisting of a few keywords that will be used to gather any relevant\n",
        "information. Do not output any information other than the query consisting of a few words.\n",
        "\n",
        "---\n",
        "\n",
        "These were the past queries, so you can vary the query that you generate:\n",
        "\n",
        "{queries}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Generates a new search query based on the critique\n",
        "def research_critique_node(state: AgentState):\n",
        "    messages = [\n",
        "        SystemMessage(\n",
        "            content=RESEARCH_CRITIQUE_PROMPT.format(queries=state[\"queries\"])\n",
        "        ),\n",
        "        HumanMessage(content=state[\"critique\"]),\n",
        "    ]\n",
        "    response = model.invoke(messages)\n",
        "    queries = state.get(\"queries\", [])\n",
        "    if queries:\n",
        "        queries.append(response.content)\n",
        "    else:\n",
        "        queries = [response.content]\n",
        "    return {\"queries\": queries}\n",
        "\n",
        "\n",
        "# Decide whether to continue to the next revision or end the process\n",
        "def should_continue(state: AgentState):\n",
        "    if state[\"revision_number\"] > state[\"max_revisions\"]:\n",
        "        return END\n",
        "    return \"perform_critique\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aae38c0085e3"
      },
      "source": [
        "## Define and compile the LangGraph workflow\n",
        "\n",
        "This section defines the structure and flow of the AI podcast agent using LangGraph.\n",
        "\n",
        "The workflow is constructed as a graph with nodes representing each stage in the process (e.g., outlining, research, script generation) and edges defining the transitions between these stages.\n",
        "\n",
        "The workflow includes two main loops:\n",
        "\n",
        "- **Research Loop:**  This loop iteratively plans and executes searches until a specified number of searches are completed.\n",
        "- **Critique and Revision Loop:** This loop handles the script critique, additional research based on the critique, and script revision, repeating for a set number of revisions.\n",
        "\n",
        "The `workflow.compile()` function transforms this graph definition into an executable workflow, incorporating memory management to maintain the agent's state throughout the process."
      ]
    },
    {
      "source": [
        "# Common libraries\n",
        "import logging\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Typing utilities for data validation and schema definitions\n",
        "from typing import TypedDict\n",
        "\n",
        "from IPython.display import Audio, Image\n",
        "\n",
        "# Libraries for text-to-speech generation and audio processing\n",
        "from google.cloud import texttospeech\n",
        "from langchain.schema.document import Document\n",
        "\n",
        "# Tools\n",
        "from langchain_community.retrievers import (\n",
        "    ArxivRetriever,\n",
        "    PubMedRetriever,\n",
        "    WikipediaRetriever,\n",
        ")\n",
        "\n",
        "# LangChain and LangGraph components for message handling and tool integration\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, ToolMessage\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "# LangChain integrations for Gemini API in Google AI Studio and Vertex AI\n",
        "from langchain_google_vertexai import ChatVertexAI\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import END, StateGraph\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# Set logging level to ERROR to filter warnings\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.ERROR)\n",
        "\n",
        "\n",
        "# Initialize agent memory\n",
        "memory = MemorySaver()\n",
        "\n",
        "\n",
        "# Define the agent's state\n",
        "class AgentState(TypedDict):\n",
        "    revision_number: int\n",
        "    max_revisions: int\n",
        "    search_count: int\n",
        "    max_searches: int\n",
        "    task: str\n",
        "    outline: str\n",
        "    queries: list\n",
        "    content: list\n",
        "    draft: str\n",
        "    critique: str\n",
        "    tool_calls: list\n",
        "\n",
        "\n",
        "model = ChatVertexAI(model=\"gemini-2.0-flash\", temperature=0)\n",
        "\n",
        "\n",
        "@tool\n",
        "def search_arxiv(query: str) -> list[Document]:\n",
        "    \"\"\"Search for relevant publications on arXiv\"\"\"\n",
        "    retriever = ArxivRetriever(\n",
        "        load_max_docs=2,\n",
        "        get_full_documents=True,\n",
        "    )\n",
        "    docs = retriever.invoke(query)\n",
        "    if docs:\n",
        "        return docs\n",
        "    else:\n",
        "        return [\"No results found on arXiv\"]\n",
        "\n",
        "\n",
        "@tool\n",
        "def search_pubmed(query: str) -> list[Document]:\n",
        "    \"\"\"Search for information on PubMed\"\"\"\n",
        "    retriever = PubMedRetriever()\n",
        "    docs = retriever.invoke(query)\n",
        "    if docs:\n",
        "        return docs\n",
        "    else:\n",
        "        return [\"No results found on PubMed\"]\n",
        "\n",
        "\n",
        "@tool\n",
        "def search_wikipedia(query: str) -> list[Document]:\n",
        "    \"\"\"Search for information on Wikipedia\"\"\"\n",
        "    retriever = WikipediaRetriever()\n",
        "    docs = retriever.invoke(query)\n",
        "    if docs:\n",
        "        return docs\n",
        "    else:\n",
        "        return [\"No results found on Wikipedia\"]\n",
        "\n",
        "\n",
        "OUTLINE_PROMPT = \"\"\"You are an expert writer tasked with writing a high level outline of an engaging 2-minute podcast.\n",
        "Write such an outline for the user provided topic. Give an outline of the podcast along with any\n",
        "relevant notes or instructions for the sections.\"\"\"\n",
        "\n",
        "\n",
        "# Generate an outline for the podcast based on the user-provided topic\n",
        "def podcast_outline_node(state: AgentState):\n",
        "    messages = [\n",
        "        SystemMessage(content=OUTLINE_PROMPT),\n",
        "        HumanMessage(content=state[\"task\"]),\n",
        "    ]\n",
        "    response = model.invoke(messages)\n",
        "    return {\"outline\": response.content}\n",
        "\n",
        "\n",
        "RESEARCH_PLAN_PROMPT = \"\"\"You are a researcher tasked with providing information that can\n",
        "be used when writing the following podcast. Generate one search query consisting of a few\n",
        "keywords that will be used to gather any relevant information. Do not output any information\n",
        "other than the query consisting of a few words.\n",
        "\n",
        "These were the past queries, do not repeat keywords from past queries in your newly generated query:\n",
        "---\n",
        "{queries}\"\"\"\n",
        "\n",
        "\n",
        "# Generates a search query based on the outline\n",
        "def research_plan_node(state: AgentState):\n",
        "    messages = [\n",
        "        SystemMessage(content=RESEARCH_PLAN_PROMPT.format(queries=state[\"queries\"])),\n",
        "        HumanMessage(content=state[\"task\"]),\n",
        "    ]\n",
        "    response = model.invoke(messages)\n",
        "    queries = state[\"queries\"]\n",
        "    if queries:\n",
        "        queries.append(response.content)\n",
        "    else:\n",
        "        queries = [response.content]\n",
        "    return {\"queries\": queries}\n",
        "\n",
        "\n",
        "RESEARCH_TASK_PROMPT = \"\"\"Use the available search tools and search queries to find information\n",
        "relevant to the podcast. Try searching different sources to obtain different articles. Try using\n",
        "different search tools than what was used previously so that you can obtain a broader range of\n",
        "information.\n",
        "\n",
        "These are the previous tool calls, so you can choose a different tool:\n",
        "---\n",
        "{tool_calls}\n",
        "---\n",
        "These are the previous search results, so you can aim for different sources and content:\n",
        "---\n",
        "{content}\"\"\"\n",
        "\n",
        "\n",
        "# Performs searches using tools\n",
        "def research_agent_node(state: AgentState):\n",
        "    tool_calls = state[\"tool_calls\"]\n",
        "    content = state[\"content\"]\n",
        "    queries = state[\"queries\"]\n",
        "    query = queries[-1]\n",
        "    messages = [\n",
        "        SystemMessage(\n",
        "            content=RESEARCH_TASK_PROMPT.format(tool_calls=tool_calls, content=content)\n",
        "        ),\n",
        "        HumanMessage(content=query),\n",
        "    ]\n",
        "\n",
        "    # Perform function calls\n",
        "    tools = [search_arxiv, search_pubmed, search_wikipedia]\n",
        "    model_with_tools = model.bind_tools(tools)\n",
        "    response_tool_calls = model_with_tools.invoke(messages)\n",
        "    if tool_calls:\n",
        "        tool_calls.append(response_tool_calls)\n",
        "    else:\n",
        "        tool_calls = [response_tool_calls]\n",
        "\n",
        "    # Defines a tool node based on search functions\n",
        "    tool_node = ToolNode(tools)\n",
        "    response = tool_node.invoke({\"messages\": [response_tool_calls]})\n",
        "\n",
        "    # Add the search results to the content list in the agent state\n",
        "    for message in response.get(\"messages\", []):\n",
        "        if isinstance(message, ToolMessage):\n",
        "            content.insert(0, message.content)\n",
        "\n",
        "    return {\n",
        "        \"content\": content,\n",
        "        \"tool_calls\": tool_calls,\n",
        "        \"search_count\": state[\"search_count\"] + 1,\n",
        "    }\n",
        "\n",
        "\n",
        "# Determine whether to continue research based on the number of searches performed\n",
        "def should_continue_tools(state: AgentState):\n",
        "    if state[\"search_count\"] > state[\"max_searches\"]:\n",
        "        return \"generate_script\"\n",
        "    else:\n",
        "        return \"research_plan\"\n",
        "\n",
        "\n",
        "WRITER_PROMPT = \"\"\"\n",
        "You are a writing assistant tasked with writing engaging 2-minute podcast scripts.\n",
        "\n",
        "- Generate the best podcast script possible for the user's request and the initial outline.\n",
        "- The script MUST strictly alternate lines between the two hosts, separating each host's line with a newline.\n",
        "- Add an intro phrase and outro phrase to start and end the podcast, and use a fun, random name for the podcast show.\n",
        "- Given a critique, respond with a revised version of your previous script.\n",
        "- Include lively back-and-forth chatter, reflections, and expressions of amazement between the hosts.\n",
        "- Cite at least THREE pieces of research throughout the script, choosing the most relevant research for each point.\n",
        "- DO NOT include ANY of the following:\n",
        "    - Speaker labels (e.g., \"Host 1:\", \"Host 2:\")\n",
        "    - Sound effect descriptions (e.g., \"[Sound of waves]\")\n",
        "    - Formatting instructions (e.g., \"(Emphasis)\", \"[Music fades in]\")\n",
        "    - Any other non-dialogue text.\n",
        "- Use this format for citations, including the month and year if available:\n",
        "    \"In [Month, Year], [Organization] found that...\"\n",
        "    \"Research from [Organization] in [Month, Year] showed that...\"\n",
        "    \"Back in [Month, Year], a study by [Organization] suggested that...\"\n",
        "---\n",
        "Utilize all of the following search results and context as needed:\n",
        "{content}\n",
        "---\n",
        "If this is a revision, the critique will be provided below:\n",
        "{critique}\"\"\"\n",
        "\n",
        "\n",
        "# Generates a draft of the script based on the content and outline\n",
        "def generate_script_node(state: AgentState):\n",
        "    messages = [\n",
        "        SystemMessage(\n",
        "            content=WRITER_PROMPT.format(\n",
        "                content=state[\"content\"], critique=state.get(\"critique\", \"\")\n",
        "            )\n",
        "        ),\n",
        "        HumanMessage(\n",
        "            content=f\"{state['task']}\\n\\nHere is my outline:\\n\\n{state['outline']}\"\n",
        "        ),\n",
        "    ]\n",
        "    response = model.invoke(messages)\n",
        "    return {\n",
        "        \"draft\": response.content,\n",
        "        \"search_count\": 0,  # Reset the search count for the next revision\n",
        "        \"revision_number\": state.get(\"revision_number\", 1) + 1,\n",
        "    }\n",
        "\n",
        "\n",
        "CRITIQUE_PROMPT = \"\"\"You are a producer grading a podcast script.\n",
        "Generate critique and recommendations for the user's submission.\n",
        "Provide detailed recommendations, including requests for conciseness, depth, style, etc.\"\"\"\n",
        "\n",
        "\n",
        "# Generates a critique with feedback on the draft podcast script\n",
        "def perform_critique_node(state: AgentState):\n",
        "    messages = [\n",
        "        SystemMessage(content=CRITIQUE_PROMPT),\n",
        "        HumanMessage(content=state[\"draft\"]),\n",
        "    ]\n",
        "    response = model.invoke(messages)\n",
        "    return {\"critique\": response.content}\n",
        "\n",
        "\n",
        "RESEARCH_CRITIQUE_PROMPT = \"\"\"You are a writing assistant tasked with providing information that can\n",
        "be used when making any requested revisions (as outlined below).\n",
        "Generate one search query consisting of a few keywords that will be used to gather any relevant\n",
        "information. Do not output any information other than the query consisting of a few words.\n",
        "\n",
        "---\n",
        "\n",
        "These were the past queries, so you can vary the query that you generate:\n",
        "\n",
        "{queries}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Generates a new search query based on the critique\n",
        "def research_critique_node(state: AgentState):\n",
        "    messages = [\n",
        "        SystemMessage(\n",
        "            content=RESEARCH_CRITIQUE_PROMPT.format(queries=state[\"queries\"])\n",
        "        ),\n",
        "        HumanMessage(content=state[\"critique\"]),\n",
        "    ]\n",
        "    response = model.invoke(messages)\n",
        "    queries = state.get(\"queries\", [])\n",
        "    if queries:\n",
        "        queries.append(response.content)\n",
        "    else:\n",
        "        queries = [response.content]\n",
        "    return {\"queries\": queries}\n",
        "\n",
        "\n",
        "# Decide whether to continue to the next revision or end the process\n",
        "def should_continue(state: AgentState):\n",
        "    if state[\"revision_number\"] > state[\"max_revisions\"]:\n",
        "        return END\n",
        "    return \"perform_critique\""
      ],
      "cell_type": "code",
      "metadata": {
        "id": "tsd4QhuDUl08"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "f7d04cda5f36"
      },
      "outputs": [],
      "source": [
        "# Initialize the state graph\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "# Define the nodes of the workflow, representing each stage of the process\n",
        "workflow.add_node(\"podcast_outline\", podcast_outline_node)\n",
        "workflow.add_node(\"research_plan\", research_plan_node)\n",
        "workflow.add_node(\"research_agent\", research_agent_node)\n",
        "workflow.add_node(\"generate_script\", generate_script_node)\n",
        "workflow.add_node(\"perform_critique\", perform_critique_node)\n",
        "workflow.add_node(\"research_critique\", research_critique_node)\n",
        "\n",
        "# Specify the starting node of the workflow\n",
        "workflow.set_entry_point(\"podcast_outline\")\n",
        "\n",
        "# Define the flow between node and stages\n",
        "workflow.add_edge(\"podcast_outline\", \"research_plan\")\n",
        "workflow.add_edge(\"research_plan\", \"research_agent\")\n",
        "workflow.add_edge(\"perform_critique\", \"research_critique\")\n",
        "workflow.add_edge(\"research_critique\", \"research_agent\")\n",
        "\n",
        "# Define conditional edges for the research loop\n",
        "workflow.add_conditional_edges(\n",
        "    \"research_agent\",\n",
        "    should_continue_tools,\n",
        "    {\"generate_script\": \"generate_script\", \"research_plan\": \"research_plan\"},\n",
        ")\n",
        "\n",
        "# Define conditional edges for the critique and revision loop\n",
        "workflow.add_conditional_edges(\n",
        "    \"generate_script\",\n",
        "    should_continue,\n",
        "    {END: END, \"perform_critique\": \"perform_critique\"},\n",
        ")\n",
        "\n",
        "# Compile the workflow graph, enabling memory to track agent state\n",
        "graph = workflow.compile(checkpointer=memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18154fa6d8b4"
      },
      "source": [
        "### Visualize the workflow\n",
        "\n",
        "This cell visualizes the compiled LangGraph workflow as a [Mermaid diagram](https://mermaid.js.org/).\n",
        "\n",
        "The diagram provides a clear and intuitive representation of the workflow's structure, showing the nodes, edges, and the flow of execution.\n",
        "\n",
        "This visualization helps to understand the overall process and the interactions between different stages of the AI podcast agent."
      ]
    },
    {
      "source": [
        "# Display a Mermaid diagram of the workflow\n",
        "Image(graph.get_graph().draw_mermaid_png(max_retries=5, retry_delay=2.0))  # Increased retries and delay"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "id": "ZAe9XSLPVft9",
        "outputId": "a470f17f-ae35-4c01-f552-8f0b9338bce1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUoAAAKFCAIAAAAlHTeuAAAQAElEQVR4nOzdBVyT+R8H8N+CbTC6uwRFAQHFxDPOVuzuwm6x4+zuBuNOxe4+u89TzxZFkFCkpWEbgwH/Lzze/pwSghtsz77vly989sT2DJ7P86vtedj5+fkEIURHbIIQoimMN0K0hfFGiLYw3gjRFsYbIdrCeCNEWxhvVGW+RIkF6bmCdEl2Vl62KI8oPJYag81m8LXZGtosA3MuT4NJFBsDx71RJQsPFEQU/Mu0duKLRbmQFj0TjiRbCeKtxmVmpkjgfARnJWGGRJ3PsnPhO7pra+mziELCeKPKE/5a8PBionk1dVNbnr0Ln8dX0FT8oNiILDhPJcVla+uxG3cyVOMyiILBeKPKkJuTfzUgDiYgBrpGaoRe3jxIe3ghEd6aaxMdokgw3kjuEiLFp7ZG9ZpsaWjJJfT19HpK6pfsVv1NiMLAeCP5SkvMuXogrvc0K6ICgp6kQ3W9w3Azohgw3kiOPgcLH11K6qUa2aYEP81481daz8mWRAEoes8+Ul7C9Nzrh+JVKtughqeWUz2t28cTiALAeCN5uXEkfsAcW6J6XBrraOqqBT3JIFUN443kAvqZjCy5XHWFGyuqHHVb6t0+Fk+qGsYbyR705zz6M6lRRwOiqpgsUq+t/uM/k0mVwngj2Xt+M6VFL2Oi2uq10Y/7lCXJrsqua4w3kr13j9MtHdVJJQoLC/P29iblN2vWrAsXLhD5UNdkhb3JJFUH441kLCU+h8kkOoaV+tG0oKAgUiEV3vBH2DnzPwYKSNXBeCMZiwoROtXTJvIRFxc3e/bs1q1bN27cuGfPnqdPn4aZ/v7+ixYtgkWenp6HDx+GOe/evRs3blzLli2bNGkyePDgx48fU5sfP34ctr179y783LRpE6wfExOzePHi5s2bEzmwd+Wnp+SQqoNfCEUy9iVGbGLDI/IBUczOzoZk6ujoPHr0aNWqVebm5kOGDMnIyLh9+/ahQ4fU1dXFYvHEiRNdXV137NihpqYGpwBfX1/4aWxsDA9FItHRo0fhdGBra9u/f/8OHTrMmDGjXbt2RA5YbEZmqkSQJuHrVE3QMN5IxgTpEr62vI6r0NDQPn36ODs7wzSU3k5OTmZmZjwej8vlMhgMXV1dmC+RSKA8NzQ0pB6OHTsW8vzq1SsosWGdrKwsSLWXlxcsghMB/NTQ0ICTBZEP+FUI0nMx3ogmhHA0a8vrm55Nmzbdt28flNWQTw8PDxcXl+/XYbPZOTk5a9asCQkJgTWpj12npaVJV4CCnVQWjYJ4Swipmu/SYLyRjLHUmCyWvD7NMmfOHAcHh8uXL0M9nM/nQwEOhTPkueg6kZGRY8aMqVev3tKlS42MjPLy8qAGXnQFTU1NUlk4PGZ+1V2oAuONZIzDYWSmSfTNOEQOIMn9CiUlJV26dAla13p6egMHDiy6zrVr13Jzc5cvXw41dlLYG0eqTlpijoZWlV21AnvOkYxBOxM6k4gcZGZm/vnnn9C0hmkDAwPoEodqNrTGv1kN+t6o1jj1EIr60p9Wrl+arMJ+NYLxRjJnYMbJFsslMNAxtnr16mXLlgUHB0dHR1+5cgVGrevWrQuLtLS0EhMTX7x4ERsbCw3y1NTU8+fPw5wTJ068ffsWSnhoh8PZ4Zsn5BZ6/vw5PCF11pA5bQM1Td0qizcLRggIQrLDZDGeXk92aSz7vmgOhwMj1bdu3YLeNegM//DhA1TLe/XqBYtMTU0fPHhw5MgRGBjr0aMHjH4FBATAOrDJggULoK4OOYfeNehOv3fvno+PD5P5tWCDlvmZM2euXr0KzXhpgS8rH98KkmLFNepqkSqCl3NAsvfHwoje06z5Osp9pcSfd/tYgrE1z7mRvD7kUyasnCPZq9lAJzpURFReZnqunTOfVB3sOUey59ZU5/DqyOp1Sxx/Onny5LZt24pdBB1jUKMudtHixYubNWtG5KOUz6VC3Z7FKr4mAvV/aBcUu+jNX2laegU3PCBVByvnSC4enEvU1GG7N9ctdin0cqWnpxe7KCMjA/rJil2kr68PXeJEPmJiYkpaJBaLS2qWGxsbfzPqLuU/O2z4Yjs1blVWkDHeSC7ycsl5/+iu4yyISnrzIE2Sne/xqy6pUtj2RnLBZJFGHQ1ObPxMVM+nIGHEW0GVZ5tgvJH8mNjwajXU+XNfVX5orPKlJUpuHY3vPNqcKACsnCP5igoRvf07re0QU6ICYiOyINv9Z9swFOMSklh6I/myrK5u68w/siYyO0sJ7gH6M94/zfjrfOKAOYqSbYKlN6ocSbHZt48nmNnxGncyVJyjX1Yig4UPzyda1+Q39lasi8NivFHleX4r5eHFpAbtDCyq8cyrVeq1FuVBmJEbESiI+5SVmSrx6mRgaKFwN0jEeKPK9upeWtirjMSYbOdGOnm5+RraLB0DNaU4DllspiBNIkiXCNNzIdJfosX2LvzqdbUsHBT0VIXxRlUjW5QXFSrKSM4RpOdCyAsvaSJLb968sba2lu1VlngaBR9Bg/MRX5ttYMY1sVH0+xljvBE9jR49euTIkZ6enkSF4WfOEaItjDdCtIXxRoi2MN4I0RbGGyHawngjRFsYb4RoC+ONEG1hvBGiLYw3QrSF8UaItjDeCNEWxhsh2sJ4I0RbGG+EaAvjjRBtYbwRoi2MN0K0hfFGiLYw3gjRFsYbIdrCeCNEWxhvhGgL443oSbY3MFBSGG9ET2lpaUTlYbwRoi2MN0K0hfFGiLYw3gjRFsYbIdrCeCNEWxhvhGgL440QbWG8EaItjDdCtIXxRoi2MN4I0RbGGyHawngjRFsYb4Roi5Gfn08Qoos2bdrweDwGg/HlyxctLS0ulwsz1dTUTp06RVQPlt6IVgwMDD58+EBNJyUlURNjx44lKolJEKKRTp06USW2lIWFRd++fYlKwngjWunSpYu1tXXROR06dODz+UQlYbwRrUCSvb292eyvrU4rK6v+/fsTVYXxRnTTrVs3SDVMMJnMjh07QgcbUVUYb0Q3Ghoa0AKHAtzW1lZlW90U7DlH/5GeLEmOyRZn5RJl5l6tQ2278Hr16kW/h3HfDKK0mCyGlh5b34TLUWeQ8sNxb/SVKDP3+uGE5FixVXV+dlYeQQqAq8n68jmLzWbYu/I9WuiScsJ4owKC9Nzz/jFeXUz0TDgEKZ6/L3zRN1XzbFW+hGPbGxU4ujayZX9zzLbCatTJKCk2+/WD8t16BeONyKt7qU71ddQ1WQQpsAYdjN49Ss8rT68IxhuR+E9iTV01ghQbi82APpH05Jwf3wTjjYhYlKelh9VyJaBvys1Mkfz4+jgwhki2KDcvD7vKlQCU3uXqC8d4I0RbGG+EaAvjjRBtYbwRoi2MN0K0hfFGiLYw3gjRFsYbIdrCeCNEWxhvhGgL440QbeFXSlDVSEtLbdHS887dG0RpdenW8kDAHpg4feZYy9b1ieLBeCNaiYgI69vfm8hN1+6tYuNivpnp4e45ZfJsoniwco5oJSQkiMhNfHwcVDq+n29nVw3+EcWD8UblFvLh/egxA5cuXnfq9JEPoe9ZLHa7tp1Gj5rEZBZUBhMS4nf6bXz27LEoS2RlZdOvz5DWrTtQG56/cOrQ4d9TU1McHZ18ho8v+pxBQYE7/TdBOLW1dX5t0Xb4sLEcTsFX0G/cvHL8eEBUdKSaGsfZufb4cb4W5pakMGl+/ptevnomFApMTc179ujfybv7vv3++w/shqVQ7R8/bhrMLOVdlLSfx44HwPP8eemBdLU+/TquWLaRp64+zXcMzOk/oLOXV7NlS9ZLnwoq59t3rL95/QlMd+vRetCAEfEJcbduXxWJhK6uHtOnzTcwMIRFEonk4KG9t25fi4+PNTIy6dVzQJfOPYk8YbxRubFZBYeN/+4tc2YvcapR69GjB78tmmFtbduxQ9ecnJwZs8arqaktXbIejukbN/9cseo3DQ0+5OH16xcbN62EYxpyGB0TBdGSPiFUd6fPHPdLk1/HjJqclJy4fsNysThr8qRZQe/fLl8xf+CA4fNbLRcIBbt3b124aMaeXUdgkzVrF2fnZK9YvglOB0+fPtq0eRWEvG+fIRmZGQ8e3N7ld4jHUy/lLZSynyVt4uri/tuClUuWzvH3O2hhblXiL4fNPnJsP5yejhy6kJycNG7CkICDe6iqu5//5kuXz0yZNNvZxQ1OK9u2r4OV4ZdG5AbjjSqodasOtWq6wETjxk2h8Xn12kU4Uh8//isy8uMu/0OODjVg0dAho589f3Lm7DGIzbXrl/T1DaCQZ7FYUFpmZmZAdKmnunTpDIfDnTF9ASyChyKh8PWbFzBhZWnjtzOgmr0jdVMhKI3nLZiWkpKsp6cfHhHarWufmk7OMN+ic8/qjk4mJmY8Ho/L4TIYDB2dMq4oWsp+lrQJ7APkHya0tLRLv2mZjbVd+3adYcLY2KR+vcbBwe9gOjMz89z5EwP6D2vbtqBrwNLC6sOH94eP7MN4I0UEiZJO29jY37l7HSagrs7lch2qVf//atVr3rx5BSY+RUbANBVgULPw1ECBOjk8m3RRmzYd4R9MaGpqxsZG79mzLTr6c5Y4S5JTcJmxjIx0iHfjRk2PHN0H54gGDbxqu3oUfbYfUcp+/jx7e0fpNJwL0jPSYSIsLAQq5551G0oXubnVvXT5rFgs/uaWpjKE8UYVpK6uUWRaHZIGE5mCTKgVQ/kpXcTX4EPzGCbgp4G+4f83KVJ5hsQaG5t+/xLQTF26bO6ggSMmTpjB52u+CXy5eMnXDuqpU+bY2zlcv3H5xMlDUJZ27tQT6sPSOweWqZT9/HnfxJV6DerJp/qOlr4odVkl+L1hvJHCgX4j6TQ0jDU1C+7Up8nXhPlw4EoPYlgEyYQJiJNAkCndhDodUHR09YqNFlTaodoPuaUeirOypIsgyT169IN/0L6Fav/e33fo6ur17jWQ/JhS9rNo5kF2tpjIAvXk8+Yug7NS0flltiN+Bo57owqCXmvpNDQvra1sYaJG9VrZ2dnQtS5d9O7ta6fCFjI0pMPCP0iv2fj02WPpOtAADnofCNVU6uG1a5cmTfGBNaHzrOjRf/NWQeUZMgnt2Os3/oS6LjyE9nzfPoNr1XINDw8lP6yU/YQGdlZWFvXkIDQs5JttK3ZjH6ixQ08edBxAHyT1DzoF4d39eI2jAjDeqIIe/n3v5q2rMbHRUD1+9+4N1ZlUv35jGxu79euXQac3dI/v3rPtffA76C2HRS1btoODe/vODZDDe/dvXbt2UfpU3h27Q5ygpy0w8NWDB3egTx56p2CYraaTC/SKw5hZXFws9LrrF9bt4VQCJeqWravXrV/2ITQYdgAGz6D17u5elxQ017WSkhKhlx42KWXnS9lPaITDz8t/noOf0P127twJ6VbaWtrwE0YKPn4MJ+UE/QjehUN30OKAfX7x8ikMFqxas4jIE1bOUQVBnRl6y9etXwqd3jBNDRpDWbRm1bYdOzfMnDUeykCogCx/rQAAEABJREFUiMLweB2PerConmdDGIs+euzAhQunYNzb13f+qNEDqJLQxMR09cqtfrs2+84YC2Va8+atR46YAPMHDBgeExsFM6FEhVPA4EE+SUlf1m1YNnPGwtWrtkGX2zTf0VAIw5DYsKFjYOwdNmn5azvYK9ikf7+hMLOknS9lP6GTz2fE+AMBu3ft3mJn5zBp4kzYT6rSAcmH8wIM6cEg2Yb1fqScxo2ZqqWpBU8LJyCodEDv4Ij/Dv7LHN5CEJFTm6PcWhiY2Kj/4PpQ/I4Y2XfLpj2uru4EVaLrATH1Wuta1dD4wfWx9EaItjDeiJ4OH9kHA+PFLrK2ttu+9Q+iAjDeqNzs7R1u33xKFFunTj1atGhT7CI1tqrcLxHjjegJOrG0CofiVRnGGyHawngjRFsYb4RoC+ONEG1hvBGiLYw3QrSF8UaItjDeCNEWxhsh2sJ4I6JtyCF5BCk+dT6LzS3HNRrwcg6IaOqwEqKyCFJ4H4MyjczLcWE2jDci9q6ayXEYb0UX91FUzVWTzWH8+CYYb0RMbLhWjup/nUsgSFFlpkr+vpDQaoBJubbCq7Wgr17fT/scIjK04hlZ8JjMchQRSH7gD5GamJ2VKXn3OLX/TGsOr3zlMcYb/V9MeFbYq0xhhiQlIYcoudTUVD5fQ02NQ5SZph6bzWaY2vLcm1XkeskYb0RPo0ePHjlypKenJ1FhODCGEG1hvBGiLYw3QrSF8UaItjDeCNEWxhsh2sJ4I0RbGG+EaAvjjRBtYbwRoi2MN0K0hfFGiLYw3gjRFsYbIdrCeCNEWxhvhGgL440QbWG8EaItjDdCtIXxRoi2MN4I0RbGGyHawngjRFsYb0RPJiYmTKaq32ML443oKT4+Pi9P1W9rjPFGiLYw3gjRFsYbIdrCeCNEWxhvhGgL440QbWG8EaItjDdCtIXxRoi2MN4I0RbGGyHawngjRFsYb4RoC+ONEG1hvBGiLUZ+fj5BiC5at26tpqbGYDCSk5P5fD6Hw4FpDQ2NEydOENWDpTeiFS0trcjISGo6NTUVfkK8hwwZQlSSql+tBtFMixYtvrkGk7W1dd++fYlKwngjWunXr5+VlZX0IRTdTZs2NTQ0JCoJ441oBZLcvHlzSDX10MbGBgJPVBXGG9ENVMUh1eTfotvY2JioKow3ohsjIyOqAIdausq2uinYc45Kk59HBOkSQXqucg2gtmzS469bb728vPJE2nGfsojyYLIYhuZcWV2gHce9UYnePEgLfJgmEuTpGnJyslX9muGVQ9tALfxNhqO7VrMeRlyNn005xhsV7+n11KS4bM82hhwetuAqW2K0+NaRmAGzbXj8n/rlY7xRMZ7dSElJkDToaERQFcnLzT+0ImzcOgfyE/DEjL4lzMj7HCLCbFctaIR7dTH9+2IS+QkYb/StpJisvDys01U9LX121AcR+QnYc46+lZEiMbTkEVTVtA05TCaD/ASMN/qWJCcvW4T95Aogj6QkiMlPwHgjRFsYb4RoC+ONEG1hvBGiLYw3QrSF8UaItjDeCNEWxhsh2sJ4I0RbGG+EaAvjjRBt4TfGEP1duny2RUtPiURCZKRLt5YHAvYQhYelN0K0hfFGiLYw3kgGunZvNXDA8H+ePnrx4p/TJ69ramqGfHi/Z8+24JAgiSSnjkf98eN8TU3NYM34+Dg//00vXz0TCgWmpuY9e/Tv5N2depKbt66eOHHwU2SEurrGry3a+owYz+MVfO08Nzf3QMDumzevfElM0NbW8WrcbPSoyerq6sW+blBQ4E7/TSEhQbAmPMnwYWM5HA71/FFRkes2LKMWwZO3a9up9Dc1b8E0FpPl7Fz79Jmjqakptjb2U6fOdapR65vV3ge/g3f6ITQ4O1sM64wYMd6zbgOYf+78yT/2+a1cvmnLtrWfP3/U1tIZOHBEh/ZdSCXCtjeSATabfeHiaXs7h43r/SGTkOFpvqMZTCY8XL/OLz0jzXfG2OzsbFhzzdrFiUlfVizf9Pve49279d20eRWEE+Y/eHBn2fJ5des22L3ryMwZC+/dv7l+43LqyU+eOnz4yL7hw8ft3X0UFv318O6e37cX+7qxcTHTZ44zN7PcsM5v4oQZV65e2Om3kVqTxWJt2bqmb+/B27b+4eHuuW79si9fEsp4Uyw2nDViYqIO7Dt98sRVHR3dRYtn5uX955vwYrF41uyJahzOurU7dm4/UMu59oLffKlnhn0TCDIPHNyzeOGaC+futGnTceOmlWW+qGxh6Y1kgMFg8Li80aMmUQ/PXzgJc+bPW66lqQUP585e2m9Ap7v3brZu1T48IrRb1z41nZxhvkXnntUdnUxMCkr1w0f3ubnVGekzAaYtLaxG+kxcsXLByBETjI1NWrVsX8+zkb19wUUFLS2tWzRv8/jJX8W+7qVLZzgc7ozpCyDM8FAkFL5+84JaBFWA3r0HNWzgBdNDh465cfMKFONGRmXcwCQ3L3fc2GncQoMHjZw4eQTUO+p41JOuAC8EZxYDA0MIPzwcPnTs6dNHA9++atG8NTyEzrz+fYfCW4Dp9u267D+wOywspMwXlSGMN5INqMRKp6GG7FTDmco2MDExNTOzCA0Nhng3btT0yNF9mZkZDRp41Xb1qFnTBVaAIhHCNnTIaOkzuLvVhZ/h4R8gG5Cca9cvQb06MTEBAiMSCaH2XuzrwpPA+YLKNoACE/5Jl7o4u1ETujp68FMoEpKy2FjbQbCpaVvbavAzOvpz0XhDEZ0jyYF6QWhYCLwp6rrD6elp0hXs7R2pCS0tbfiZkZlBKhHGG8kGn68pnYZKKbRF27RrJJ2Tk5OTlJwIE1OnzIG69PUbl0+cPMTn8zt36gnNY6i3Q+m6b78/tLGLPie1ydZta2H9qZPnOLu4cTncI0f337p9tdjXzchINzY2LWkPqZY8KSzzC/77gUuAFz2PUJtn/jef0J73nT7Gw73e3DlLDQ2M4DzVu2+HoitIzw5fVe51xzHeSPYgcq6u7r5T5xWdSUUFirsePfrBv+TkJCiT9/6+Q1dXDzrYYD40xTt26Fp0E109fYj95T/PDRro07r119jAuaOk19XR1YMeOyI7RZ9NUDhNFcJSt25fgz2EZggVY+h0IIoEu9aQ7EGVGyqx5uaW1ta21D8oMKGBmpmZef3Gn9THS/T1Dfr2GVyrlmt4eCiTyXR0dIqPj5WuD5V5FputraUN5SHkB/q6qWcWCAQP/75X0r03HB1qBL0PhO4u6uG1a5cmTfH5pjOsXCI+hqX9W9OGmj/8tLayLbpCTk42l8uTFtFQyyCKBOONZK+Tdw9oIa9eswiq6FB9PRCwZ9iI3u/fv4WQb9m6GnqtYX5MbDTVv+XuXtDMhqjfu38Lesg/f/4ES6FfbdLkERBmNTU1CO3VaxejY6LCwj7MnT8FGu1QCY+M/Pj9p9C8O3aHmctXzA8MfAVd8f67t0DjmfkT9+ODsnrduqUfP4bDCJ//rs0WFlZQKym6Qk0nl7S01D+vnE9KSjx77sT74LdQGQkraIdnEgWAlXMkezDEvWG9/65dWyCi0NEFnVLLlm6AghoWrV61DUaJYdgM2tsw7j1s6Bhq/LnpL79C8xV63WCsGOr2Li5u0CMNjXNYNGP6b2vXLRk+ojesDw11SNTbwFdjxw/es/voN68LfXirV27127UZxuGgwG/evDX0vZOfAOPYcDaZM3cyDOY5ONRYvGjt13b7vxo3btqn9yD/XVt27NzQoL7X7JmLT546BL0DVH2EVDW8xxj61uv7qQlROQ06qPpNiBYumgkdaevX7SRVRCzMO7vto89ye1JRWHojRFsYb6S6OnVpXtIiqGYT5YfxRqprl//hkhbp6ep7eTUjSg7jjVSXmak5oTWMN0K0hfFGiLYw3gjRFsYbIdrCeCNEWxhvhGgL440QbWG8EaItjDdCtIXxRt9S47F4GhW/BAKSmXxiZMUjPwEv54C+pW/CiQqV5SWNUMUkxWX95Ne1Md7oWyZWXA6HmSvBCwFUsZT4bLtaGuQnYLzRdxikfjv9q/uiCao6oS/SY0IFbs10yU/Aq7Wg4n2JEl/aG1uvrZG2gRpfl02wMV4p4NecHCNOSciOCc3sNt6C/ByMNypRerLk2Y3kmPCsnOz8bGEuUWy5eblMBvOba6FJ5UgkLBaLWcJSxWFsw8vPy7d34f9kuU3BeCOaaNWq1YkTJ/T09L5flJGRMWTIEDabfeDAAenNDFQBtr0RHcTFxXG53GKzDV6/fg0JDw8Pnz17NlElGG9EB0FBQbVq1Spp6bNnz5KTk2Hin3/+8fPzIyoD443oAOLt5FTidcVfvXpFTYjF4vPnz9+4cYOoBow3ooNSSu+oqKj4+Hhpl1tCQsL27dthJlEBGG9EBxDvmjVrFrvo7du3VM1cKjIyctasWUQF4GfOkdKLiYlRV1fX1S1+JAna29nZ2aTg3rsFg0RMJlNfX19B7gEmbxhvpPTevXtXSr/ao0ePtLW1+Xz+hQsXiIrBeCOl9/79+1L61S5evCidnjx58vr162EAnKgGbHsjpVd66V2UQCB48+YNURn4qTWk9Fq0aHHu3DmogZe5ZlxcHBTdhoaGRDVg5RwpNxji0i70IyubmpoSVYKVc6TcShkS+15sbOz06dOJysB4I+VWrnibmZlBR7pIJCKqAeONlFu54g1Onz4NQ99ENWDbGym30r9M8j1jY2OiMrD0RkoM+tV0dXU1NTV/fJPbt2+vWrWKqAaMN1JiPz7iLVWtWrXHjx8T1YCVc6TESv+8WrGsra0DAgKIasDSGykxKL3L1a9GKVdlXqlhvJESK2+3OWXr1q0HDx4kKgDjjZRVZGSkvr5+BYpiqM9DrZ6oAGx7I2VVsaIbtC5EVACW3khZVTjeIDMzUxW+TIXxRspKIBBUON4bNmyQXl+RxjDeSFnp6elVOKL//PNPeQfMlRHGGykrKLqhfk4q5MKFCxwOh9AdxhspqwrHWygUfnPtVLrCeCNlZWpqKhaLU1JSSDlt37792rVrRAVgvJESg/bzu3fvSDlBt7mnpydRAXitNaTEdu7cqaam5uPjQ1BxsPRGSqwCze+srKzQ0FCiGjDeSIlVIN7Xr19XkQ+cE4w3UmomJiY5OTnl6gYXiUTNmzcnqgE/c46UG9W71qRJkx9cv3fv3kRlYOmNlBvUz8v19a8HDx4QlYHxRsoN4v3jY2PBwcE7duwgKgPjjZRbuXrXMjIyevbsSVQGtr2RcjM2Ns7NzU1KSjIwMChzZc9CRGVg6Y2UHvSu/WABfvfu3dTUVKIyMN5I6Tk5Of1gvKdPn/6DNxukB6ycI6UHpfeZM2e8vb2haZ2VlVXSZcwTEhLGjx+vOncgIviZc6S8Bg0aFBISkpOTA9PS0JqZmV24cIGgQlg5R8oqICAA+tWYhag5eXl5pdS9Hz58CKcDokow3kiJ9erVi8/nSx8yGIz69euXtDKMeEP+iSrBeAHn4j8AABAASURBVCMlNnjw4IYNG0obmFpaWvXq1Stp5aZNm5b3jkXKDtveSOn16NHj48ePUHRDw/vAgQN6enoEFcLSGym91atXW1hYQEFlZWVVUrYDAwNv3rxJVAwOjKFiCNJzGQyiLMxN7EYOn7Bp06YGnk2FGbnFrnPu9BWomZe0VGGx2AyuesXLYKyco//7EiV+eiPl0zuBvik3PSWH0Ehubi4LOtiV6KRVSM+YI0iVONXTqt9On5Qfxht9FROededEQuPOJjpGHLaaksWAxjKSc2IjRJ/eZnSbYFHesxPGGxWIChE9vJDU3seSIIUUGSQIepTSc0r5/kDYtYYKPLuV0nKgOUGKyrom38KR//bv9HJthfFGJD0pJzUhm8PDg0GhaWizo0NF5doE/6KIJMfnWNbgE6TYoL8zV1K+pjQOjCGSl5sP3bMEKTb4M6V9yS7XJhhvhGgL440QbWG8EaItjDdCtIXxRoi2MN4I0RbGGyHawngjRFsYb4RoC+ONEG1hvBGiLfxKCaKhS5fPtmjpKZGo+gfpMd4IVYEzZ4+vWrOIyBlWzhGqAiEhP3pP8p+B8UYVsWjxLAaDYW1te/zEwd/mr2zU6JeQD+/37NkWHBIkkeTU8ag/fpyvqakZrAk15N17tt25ez0lJVlXV69Z01ajRk5UU1ODRSVtkpubeyBg982bV74kJmhr63g1bjZ61GR1dfViXzcoKHCn/yZIC6z5a4u2w4eN5XA41E5GRUWu27CMWuQzYny7tp1Kf1OlvC68ix07N9y4eSU3V9L0l5awaMHC6adPXtPT04dFBw/tvXX7Wnx8rJGRSa+eA7p07kk9YbcerQcNGBGfEHfr9lWRSOjq6jF92nwDA8Mp00a9evUcVrh69eLVPx9Kd1jmsHKOKgLyGR4RCvlctWJLrVqu8fFx03xHM5jMjev916/zS89I850xNju74MvJh4/su3b90nTfBX/8fmLalLm371zbt98f5peyyclTh2Gr4cPH7d19dOaMhX89vLvn9+3Fvm5sXMz0mePMzSw3rPObOGHGlasXdvptpNZksVhbtq7p23vwtq1/eLh7rlu/7MuXhNLfVCmvC4suXDwNJ6ad2w8YGhr57dpM/r1voZ//5mPHAwb0G7Z3zzHI9rbt66DlT23FZrOPHNtva2t/5NCF3/cc//DhfcDBPTB/2ZIN1R2dfm3R5uzpG/LLNsHSG1VMPiExMVFbNu/V0daBh8dPbINCdf685VqaWvBw7uyl/QZ0unvvZutW7SMiQu3tHOp5NoT5FuYFOWQUXu/z/IWTJW3SqmX7ep6N7O0dYL6lpXWL5m0eP/mr2Nc9cfIQh8OdMX0BhBkeioTC129eUGtCUdy796CGDbxgeujQMVDwQjFuZGRcypsq5XWvXrvYxKu5d8duMD1i+Lh3795ER3+G6czMzHPnTwzoP6xtW++CrSysIMNwjujYoSu1oY21Xft2nWHC2Nikfr3GwcHvYFpTU5PFZqtxODo6ukSeMN6ogqysbKiMAaghO9VwpoIKTExMzcwsQkODIauNGzVdseq3JUvnNG3ask6d+lCvLnMTOOihwId6dWJiAlR9oVqrrq5R7OtCYqEYpLIN2rTpCP+ka7o4u1ETujoFty4RioSlv6OSXjc/Px/q+d4duknXbNKkxfMX/8BEWFgIrOlZt6F0kZtbXSi9hUKhhkbBtvb2jtJFWlra6RnluxbiT8J4owri8zWl0wJB5ofQ4DbtGknn5OTkJCUnwkTr1h00NPhQxK1c9RuUqNBqnTJ5NjRZS9lk67a1129cnjp5jrOLG5fDPXJ0P7Rdi33djIx0Y2PTkvaQx+NRE1R9gZR1ze+SXlcgEECG1TX+f4rR/vf8IhQK4OdUaGX8ewly6sriySlJVLy5XG7Rl6jky8djvJEMQORcXd19p84rOlNa5Hp5NYN/IpHo0eMH23esX7t+6YplG0vaBE4Bl/88N2igD5wXqJlwIijpdXV09aiA/bxSXpfqCMzKypKunPFvIUyda+bNXQYNkKLPZmxkQhQAdq0hGahZ0wXaoubmllD3pv5BaQZdxLDowYM70AFGCqKr3qJ5a2iURoSHlrJJXl4eJE1aPELJ+fDveyXdbMPRoUbQ+0CxWEw9vHbt0qQpPhW7iXcprwslMLSc3we/la784MFtagLq3hB+GBSQvgt4Bqjk/0iHWSXcQQTjjWSgk3cPaKmuXrMI6tvQTD0QsGfYiN7v3xfk4dTpI9DwhnGgmNjoFy+f3rl7w829bimbQFogtNCVFR0TFRb2Ye78KQ0aeEFpGRn58ftPoXl37A4zl6+YHxj4Cs4j/ru3QFcW1aFdXqW/Lozn3b17A0a/YCn0/MPIGbUVdJJ5e3eHObCIeoPQk/8jn1eBTgfoaID3DucUIjdYOUcyAOPVG9b779q1ZdLkEdDRZWtbbdnSDTBwBYt+W7ASRowXLp4JdV0onBs2aOIzYkLpm8yY/tvadUuGj+htamoO49g1nVzeBr4aO37wnt1Hv3ld6JBbvXIrDFPBoBoUm82btx5Z+OQVU8rrDhs6JiUlCZZyubyWLdsN7D8c+guh8xu2GjdmKmR11+4tSUmJ+voG0JU4Yvj4Ml+rW7e+0BkB7/3MqRvSrkGZw3uMIRL+RhD4ML1FXzOCSgAFeGZmhq7u15uHQ13j9JmjMGpNKlFyrPjv8/F9Z1r/+CZYOUeobIcO/9F/YGdoWUDl/MFfdyDbbdt4E4WHlXOkQubMmxIY+LLYRR07dBszenJJGw7oPyw7W+znvyk5OQl6xaGDcPCgkUThYeUcqVDlHJrH2TnF38cHBueln5ZRTBWonGPpjVQINVanOjDeCNEWxhsh2sJ4I0RbGG+EaAvjjRBtYbwRoi2MN0K0hfFGiLYw3gjRFsYbESaToamLR4KiY7AYusblu6wqfmMMEX1TTmSwbC5phOQnJU7MZJfvYm0Yb1V3/vz54aP7auiQHHFFrmGEKo0wQ2Jur16uTTDeqigsLGzp0qXXrl0jBd+U0li7dm3jDibXA6IJUlSf3gliQgUujbXLtRV+IVRVZGVlXbp0ic1md+nS5cKFC7m5ue3bty96md7YiKzbxxO8uphqG6ixOZV8xV5UoozknLiPok/vMrqOs2CU88+C8aa5Dx8+QFndrl27GzduPHnypF+/fnZ2diWtnBgtfno95WOQQM+Ym56cTZRZXl4ug8FkMCr1PFV4E6V8hhRhEOlkheibcoVpkhqeWvXb6ZPyw3jT04sXLzw8PCIiIubMmTN48OAOHTqUa3NRZh4cGkSZ+fr6Dho0yN3dnVSivXv3njx5MicnB2IFkeZwOJqamvBTIpEcPXq0Ak/IYjM43Iq3oHE4hD4yMzN5PB6TyfTy8mrZsiXE29LSsmJHlbqm0nfK/NKsgYWVkTpfXhchLdaESaPu3r8eHh5OFdfiHJIhSCIFV3Q1qeQ9oWDprfTEYjE0oRcuXHj37t0rV67ANJQecr3vJCoF/BWg2zI1NVU6R11d/f79+6QqYM+5Ert8+XLPnj2hdQ3TvXr1unPnDpTeVJ2QqLzbt28nJCSQStesWbPatWtLH0LxCc0EUkUw3krm48ePixYtOnu24A7S1JiWi4sLTFM/kRS0SiIjI0lVmDx5soWFBTVtZGQUGBjo4+MTGxtLKh0LjhWCFBv0x0Ke37596+zs/PjxY2jIwZhW4Z09bPX09AgqDvRE1KxZU0enCi5+qqurC5VzSDUprKs3bdrUyspq6tSpQqGwTp06pBJh21txvX//PiQkpHPnzn///TdUNXv37u3g4ECQkoB2E/SiF52zc+fO69evL1mypNKqWhhvhfP06VNPT8+YmJgZM2ZApLt06UJQ+cEJESo7xsbGRJF8+vTpt99+gx2bOXMmkT9seysEqEmKRCKYaN68+bFjx0jhUMqhQ4cw2xVWhW3vUtjY2Ozfvx9+wh/63r17RM6w9K5KWVlZ0Ne9bNmyGzduQOsa2mwwpkXdLB79pCNHjnh5eVlbl+OeHpUJTugLFiyAUUyoq8tvpAPjXTUgz35+frNnz4Z6eHBwcI0aNQhSPXAYQMhnzZrVtWtXIgcY78oD7a7ff//dycmpX79+f/31l7m5eSkf/0Y/STHb3sWC6hscG0uXLjU1NSUyhQNj8gWV7TNnzkBvmZub2+vXrw0NDTt16gRjWlBpxDEtuVq1ahUMNMA5lCg8GDmztLSEkTPof5HtyBl2rckFjGmdOnUKJmDwMzQ0tHHjxqTwr+jt7Y0fKasc0HelFEU3BVJ98eJFsVjco0ePt2/fEhnByrkswQB1o0aNUlJSJk6cCK0pGPkkCJXHx48fYeQMBsZlMnKGpffPysjIgF5QmGjXrh01pqWjo3Pw4EHMdtW6efNmfHw8UTa2trYHDhyQ1cgZlt4VJBAI+Hz+2rVrL126dPz4cagHSiQSNhu/YKsoRo8ePXLkSBiYIMpJJiNnWHqX261bt6Di/fLlS5ju1q3bnTt3qDYeZluhtGzZ0sTEhCgtTU3NjRs3tmrVqlmzZtQ3iCoAS+8fEhMT4+/vD1Wm4cOHQzc4HDdWVlYEoUpR4ZEzjHeJqDGt1NTUUaNGPXnyJCEhoXXr1kWvPYgUGbS9oYNKqQvwop4/fw5dblBbHDFixI9vhZXzb8GYFnUBo7CwsIiICKgawXT9+vVhTAuzrUSgQ+Tz58+ELio2coal91d//fVXvXr18vLyfHx8IMl9+/YlSJlBvGGQkn5tKGrkzNXVdcaMGWWurNLxhoo3vH09PT3oKrO2tt6wYQOLxarkS+ciVAFQwfTz84PW+C+//FLKaqoYbxhygG7JLVu2nDt3bv/+/ZaWllBoM5nYTqEVmrW9v5eRkQEjZzweb/ny5VAsFbuOyh3TAQEBV65cgYlOnTrBEQDZJgW3yMRs083Dhw9hvIPQl5aW1qZNm2D8D7p+qQ9WfU/lDuvAwMBq1arBBH5bi95q1KhhaGhI6A5Gc5KSklJSUopdqnKfxFi9ejVBKqB3795E5alc6Z2eng4D2gTRnZJ+5ly2VC7es2fPfv78OUF0R7Nx74pRucq5jo4OfuNaFSj7Z85lQuXivXLlSoJUALa9Cba9EV1h25tg2xvRFba9Cba9EV1h25tg2xvRFba9Cba9EV1h25tg2xvRFba9Cba9EV1h25tg2xvRFba9Cba9EV1h25tg2xvRFba9Cba9EV1h25uoTrzbtWuXmJhIXXTp6tWr8BOmmzZtumnTJoLoCNveRHUq53Xr1s3Pz6cuukT9NDIyGj58OEE0hW1vojrxHjRokJmZWdE5rq6utWvXJoimsO1NVCfeTk5OHh4e0svCmpqaDh06lCD6wrY3Uame8379+kkLcDc3NxcXF4LoC9reeB84FYp3rVq13N3doQCHkPfp04cgWrt27VpcXBxRbao17t2/f39DQ0NnZ2dsddPeqVOnoqKiiGr70YGxNw/SQl8VXCoiXIytAAAQAElEQVQ9ITKLKDFuZ5etzFym/6wworQ0tNSYbGJhr16vnT5fm0VQETBEIp1++vQpNWFubn7hwgWien4o3pf/iNMx5Do31jcw5+H9PKocg0EyUnLSknKOro3sOtbCwBw/pfN/1apVCw8PLzqHy+UOGTKEqKSy433eP8bEVqNWQ12CFIauMQf+2dS0u+D/uXkPI/NqPIIKQY/axo0bxWKxdI6FhUWXLl2ISiqjLH7/JEPflIfZVljth1k+/jOJoH9169YN8ix9CEV3jx491NTUiEoqI94fgwTahlj3U1xsDkOYmZsUm01QIRaL1bNnT0g19RDGxiDwRFWVEe+8XALtbYIUmIUDPyUe4/1/EG+qAIeod+3aVZW/QVRGvJNioQ2jcjcAVy5iYW5Odh5B/2IymZBwSDUU3VAzJypM5b4QihSNMD03I1UiSJOIMnOzs3KJLNjr/1rHPtLDwyPwQSaRBQ6XBe0gvjZbQ4etb6I0LXmMN6oaSTHZH15lhr0SMFgssSiXzWGpcdkFg34y0sSzYDAs+JWEyAKbky8WZkvEEpJHhBliqxr8GnU0HT00iWLDeKPKlpaYc+dUokhIGGw1fVsDdR0uUSp5kvz0L4LH1zPun02s86ueezMdoqgw3qhS3T6RFPY6w6iavqk1nygnJpuha6YJ//Jy84OeJz29kdJuiJmlgyKepPAzaKjyBKyITM9Uc2hspWOirNkuislimDkZ2niY3Tr+5eW9dKJ4MN6oMsAI67ZpoYbVjHRM6RDsotTU2dYeZu+fixQw4RhvJHf5+cR/drhzKzt1bdoOQZvWMHz3VPTX+WSiSDDeSO4Clkfa1zeXXae4gjKvafgpRPz+qWyG4mQC443k6+bRRD1rPa6mSnzq29zZ+NX9zOQ4RblPBsYbyVFsRFZkiEjLSIOoDL6x1vXDCUQxYLyRHN09lWjsoE9UiaY+T5xFIt8LiQLAeCN5iXwvYnI4fD2V+0qSsaPBizsK0YuO8a4kXbq1PBCwh6iSoCfpahqK+4m0V4E3py9oIBCkElnjaap9ic5KT6r6FriqxLtr91axcTGk6owbM7VhwyZlrlbl+ylDH98JtI1VqNVdlKahRvgbAalqKhHv+Pi4tDTZn6TLpW1b7+qOTqWvowj7KSux4VnaRuosNRWtHmob8z9/qPqLjsr+M+dv3rzcsnXNp8gIc3PLsWOmHjy0t5q945TJs2FRyIf3e/ZsCw4Jkkhy6njUHz/O19S04L4C586f/GOf38rlm7ZsW/v580dtLZ2BA0d0aP/1+lg3b109ceIgPKG6usavLdr6jBjP4xU05xYtnsVgMKytbY+fOPjb/JWNGv1y4+aV48cDoqIj1dQ4zs614fktzC1fvHw6zXcMrN9/QGcvr2bLlqyXSCSwV7duX4uPjzUyMunVc0CXzj3LfF+XLp89eepwbGw0l8tzq11nwvjpxsYFN8FISkrcsXPDk38eMhjMunXqw1uG+RERYcN9+ixfumHXnq3qPPWdOw5A5bxH936DB/mcOHko4ODeBfNXbN+xHnZAV0dv6JDREP6i+wm/rh/ZJUWWnJCdz5BjtqNi3l++vgN+5kpyHKvV69x+qr5ewbF04OhcGGCv4djo9r0DaRlfjA1tunlPt7FyhUW5uZJzlzc+f30lPy+vVo0mDvaeRG446moRb0Wkqsn4DyAWi+f/5qvB52/ftm/KpNkQZsgDo/ADDVA0TfMdzWAyN673X7/OLz0jzXfG2OzsgsuMsNlsgSDzwME9ixeuuXDuTps2HTduWvnlS8HowoMHd5Ytn1e3boPdu47MnLHw3v2b6zcup15LTU0tPCIUThmrVmypVcs16P3b5SvmN2jg5bcjYNXKLVki0cJFM2A1Vxf33xashAl/v4NzZi2BCT//zceOBwzoN2zvnmOQ7W3b10F0S39fr1+/WLd+GeQTNlm5YnNaeuripQUnLDhTzJ4zKSYmavGitXDigDc7Z97kvLw86uJe+w/s6tN70IzpvxV9Khar4M3CCWv92p3nztyCN7t67eLIyI9F97Nd205EyQnSJCw1eV2kOSU1zu/3cUwGc+zwHWOGbxcK0/33TciRFBxL8OuN+PQq8vPbKeMOLJp1RUND59jpZdRWt+7tf/z0bOf2U6aOO2Bn637j7u9Ebthcllggm++u/wwZx/vvR/fT09OmTp7j6FDD3b3upIkzoXCjFp2/cBJyPn/ecnt7B6catebOXgphuHvvJrUUctK/71Ao92Cd9u26wMOwsBCYf/joPje3OiN9JlhaWDVs4DXSZ+KNG38mJBTc+TGfEMjV7FmLYQUdHV0rSxu/nQFDBo+C8rymk3PPHv3Dwj6kpCTDuUNDo+Bzzlpa2nw+PzMz89z5E5A6KDDhOaGQbNvG+/CRfaW/r4iPYVwuF1IH1YFaNV0WLlgFVQOYD0VuaFgIBLiOR73atT18fefDbiQmfqG+t+zu7tm+XWd4v988G+R/0EAfAwNDDoczcMAIqIzcvHWl6H5KLxWmvARpBV/hJvLx9z+n4Tc8oNdSMxMHK4ta/XouSk6JfvP2FrU0O1sEGeZy1DkcXp3a7RISP2ZnF9STn73606VWs/p1OhkaWDWu36N6tQZEnjg8ljCjihMu43hDKaTJ17S1taceurq6Q/Co6aCgQKcazlqaWtRDExNTMzOL0NBg6bb29o7UBBzf8DMjMwNiEBIS5Fm3oXQdd7eCi9SHh3+gHlpZ2ehof/22raamZkHhOXcyVG6792yzavXCgifJ+HZ8As4acO4o+pxubnXhNCEUljZQ6eHuCeedSVN8Ll46A11f+voGEHKYD7sHEZUGGE5qixaupirtpOC2R64lPaHjv+1wKOctzK2io+l3L0sGiy2vynnk50Bri1rq6l+PJT1dU309i+jYEOohpBeCTU1rqBccS0JROrQHE5M+w7lA+iTWls5Ennia7DxJZVzITF9fn1HCJ35l3PaGohtq5kXnaP8bP6iRfggNbtOukXRRTk5OUnKi9OG3RVZ+flZWVm5u7r79/gcCdhddIt2Kz///5TKgLb102dxBA0dMnDAD5r8JfLl4yWzyHaGwoD9zKjQT/v2NULcNTU5J0tAosZsXagTbtvxx5Nj+Xbu3ZmxYXrOmC7S9IeFw+uDx1EvaqujufYPqPvg6ra4O5zJCLzw+48sX2Vwp5XuiLEFMXPCsRf8ficjNzUnP+HpUsNnf133yoUiH/9SKLOJy5durn5Ek1tCujOspJCcnS299+w0ZvzxEFDJZdA4EnpqAYx0Kc9+p84ouhd6yUp4NMgBV1u7d+nbs0LXofF29Yj4IdenSGShjhw8bSz0UZxXfb0lFbt7cZfZ2/6kzGxuVcbPYatUc589dBqcb6Dvc+8eOufOmHD96WVdXD84X8MtllPMLEyKRSF3963kBnsHUxIzQi6YOOzdHTOSDx+PbWbv37PKf0zeHU9qxpFZYnovE//++h0gkx1Nqbk4em8tkVvUdomRcfbKwsII8R8d8vXUbJEE60gMlHlRBoTsdSkLqH0QC2p+lPBuTyYRKLHQvSzeB+jyLzdYurL1/IzsnW9oQIAX97VfIvyUzhZqGJgDUh6FNLn1OqF/AhqVfLhdaFm/fviaF19aFPgU4icD7Sk5OcnCoAVX9d+/eUKt9/Bg+esxA6DYnZXn16hk1AY0CaNFYWdl+s5/KTltfjS23osvGyiUx+bOBvqWxkS31D9oC2lqlHUtqbI6erlls3AfpnJCwJ0RuJOJcM9uqH/OXcbwbNmgCBTj0RcMhC9ne6b9JGuBO3j1EIuHqNYugih4VFXkgYM+wEb3fv39b+hP27TP43v1b0PX1+fMn2HDFygWTJo8QCIr5wEBNJ5enTx9BDuPiYqHjXV+/4HWDg99BbYI6HTx69ADiB010b+/uUOGHynxMbDT0jU2fOW7VmkWl78bjJw/nLZgGHYFw5oLdOH36KJS30H0AI2HQ8F67fuk/Tx/B+4VefXG2GHoESn82OEdAlyGsD29q05ZVpOBe8+3gp3Q/pf2Rysu6pkZ8mLw+mNnQs5tYLDx6ekl0TPCXxMjrt/eu29bvc3QZx5KHa5vAd3cfPT0bGxd6969DMf+21eUhLUGgb1r1X5KT8QkW+pygV3n7zg0+o/pB7RcaqHDoczgFDR4Y4t6w3n/Xri2QTzi+bW2rLVu6oZTOJ0rTX36dO2fpkaP7YGAc6tUuLm4wrsbnF3PFjwEDhsfERsFgG/Q/e3fsDiPMSUlf1m1YxmSxWjRvXb9+451+G2HwacN6v3FjpkIP367dWyBFsMONGzUdMXx86bsxcMBw6Jvx89uUmPSF2g0Ye6Mq5CuWbdq6fe2ixTNZTBb00s2bs4z9A8XWKJ+JW7ethYE9I0PjpYvXQYc8zKxevSa1nzBCCyN2RJnB78a8mkbGF6E8vi4GQ9xjhu+4dG3b9j2joAZsalxt2IB11OB2KVr/6iMQpl68siUvP69mda+ObSYcODYHpokcCJOEjp2MSVVjlF4VDFj+6df+5lDRIj8sLT2Nx+VR/WQwrN2l26+jRk7q1rU3QYVOnzm2fcf6m9dlVjN8eC7B2olXs742UTDvn2QEPhUb2ukRFZOXk/clNKHPNAtSKbp27bp161YrK6vvF8m49IZR5YGDutTxqD940Ego3I6dCID2M5TABKkep/pa988l6pprs7mqdRPyhPCUmp4KcQl0GccbWrarV23bvXvrpCkjmAxmNYfqa1dvL73/TEFA8x6aAMUusra22771D4LKr0kXw5f3k81qGRW7NDY+dPue0cUuYkC9soS7XzWs29W73UQiIxGfXu496Fvsory8XDiGi72zQv06nTq3n1LsVjlZEkGS0K1p1dfMiTwq50pKLBZD33uxi+BvXGxrX0EobOWccmZ7LN9Un6NRTEECo4zZ2cV/mig7O0v60ZRvsFhqJS2qAOjmoIbEi10EDftihzxL2YfE8BS3xrzKvIFJ5VXOlRe3EEGy5u1junt+eK1fbb9fBD2s0k+efaOk+TLHYrFl+FrJkel6BkRxbk6El3NA8qXGZXQda/HxH5p8ib0U6fFCiVDYsq8CNUUx3kjuzO15nUaaRjyhc8LT4gR5WYLeUyupt/wHYbxRZTAwU2vd3/D9nU85WVX/NUmZS/qUypQIu44xJQoG440qiaWj+tCFtpmxSfEhibkSuXyYpPKlxwvC/v5sZceC6glRPNi1hioPT4PZbZzZmwdpf134bGirq67N5esr5XVUJdm56QlCYZJQR5/Rc5KFjqGCDi1hvFFlc22iA/9eP0gPepIS+UpsYKWdn89gc1lqPDaMQxGFBMNj2VkSiViSm5MnSsuCwW2bWnyvfvomNgp9esJ4o6pRu4k2/MvOyosMFibH5WSmSkSC7KwMeX1F/Cdp6XKY7DxNA7auIcfYWtvEWjnGUDHeqCpxeEwHN03iRpA8lBFvHQMOg/Z3dlRyXA2mwtZpZ/dS/wAAEABJREFUUdUqo+ecwcxP+5JNkAJL+JylCp8aRhVQRultUU1DkKYodzNFxWKrMQ1MOQSh75RRetdpqfv6XrKoqq/nikry4Ey8gxufo46fX0DFKPuwGDjX5uKuyNjwqr/lAipKLMy7eyLOopp67V90CELFKbvnHPo2hy22u3U04dqBaHtXzcwUBR26+EG5hV/iVer+Qo46Kyk2S9tADUaPnTwr6ZtVSBn90MAYZKFlP2P49yVanJuj3NfxXLt2bYcOHZyd5XsJe3mDvjR1LRaOaaDSlW/c28hC6b8RbV9L39JB09RW5e4pj1SQyn2sZfz48QQh1aByPa7v3r1LTaXJPbQRKp3KxXvbtm3BwcEEIRWgcpVz6FTT1dUlCKkAbHsjRFvY9kaItrDtjRBtYdsbIdrCtjdCtIVtb4RoC9veCNGWylXOXV1d9fRU7o7TSDWpXLzHjh1LEFINKlc5f/PmDba9kYpQuXjv3LkT295IRWDbGyHawrY3QrSFbW+EaAvb3gjRFra9EaItbHsjRFvY9kaItlQu3qdOnQoLCyMI0YWdnV1Jt+VQucq5iYmJlhbe2QPRR0RERH5+8TcXwbY3QrSFbW+EaAvHvRGiLRz3Roi2sO2NEG1h2xsh2sK2N0K0hW1vhGgL294I0Ra2vRGiLWx7I0Rb2PZGiLaw7Y0QbWHbGyHawrY3QrSFbW+EaEtV4u3h4cFkfq2q7N27l/r6u5ub2++//04QoilVqZx7eXnBT0YhakJHR2fEiBEEIfpSlXgPHToU8lx0jpOTE5V5hOhKVeLt6enp7OwsvSSVtrb2oEGDCEK0pkI95wMGDIBUU9PVq1dv1KgRQYjWVCjeDRs2rFmzJhTgWHQjFaFa497Dhg3T0tJydHTEVjdSBWUMjOWI85/fSkmIzMpMzyV0YNq9/gYtLc0jaz8T5advosZiMywdNZzq4ZXbUTFKi3fCZ/HpbVHuzQ2q19Pl8VmEJowJXTCYzKQYUWJMztmdMV3HmhOE/qvEeEeHZT26lDRgbjWCFJiesRr8DHmWfmF3bKeRZgShIopve+flkgdnv7QcgAWCcqheV9vEWv3ZTfyqDPqP4uMd9UHI4bGgXUeQkjCxVQ95nk4QKqL4eKd8yTG1VSdIeeibcllMRgl3kkMqqvh4iwW5khw8UpRMUlx2ngT/auj/VO4LoQipDow3QrSF8UaItjDeCNEWxhsh2sJ4I0RbGG+EaAvjjRBtYbwRoi2MN0K0hfFGiLYw3gjRlmJday0uLnbs+CFt2jU6eeowUUgLF830nT72x+cjVIUUq/T+88q5T5/C167ebmVlQxSSt3d3SU4ONb1o8ayGDZu0a9vpm/kIKQjFindGRrqJiZmbWx2iqOp5NpROh4QEQby/n4+QgpBZvL07N+vfb1hk5MdHjx9kZYk8PRvO8F2go6MLiyQSycFDe2/dvhYfH2tkZNKr54AunXvC/IiIsOE+fZYv3bBrz1Z1njpbTS0w8BXMb9HSc6TPhP79hr5583L33m2QIgaDUdPJZeTIiTWdnGGFM2ePHwjYPX3a/HUblrVp3bFD+y5Dh/das3rbkSP7Qj4E8fmaI30mmptbbt26JvLzRzMzC99p86kNS5GTk7Nvv/+165cyMzMcHGqMHjnJxcUN5nft3mrggOH/PH304sU/p09eX7tuCaywft1O2ElYunrN4u071l84dwcq59R8UnAL8Zebtqz6/PkTvPSI4eOOHQ+wt3PwnTbvffC7seMG79xxwKlGLepFBw7q6uXVfOyYKTCdmpqyw2/jq1fP0tJS7e0d4Tfg4e5JEPoJMmt7s1jso8cOwBF5+uS1XX6HPnx4v3X7OmqRn/9mOMQH9Bu2d88xyPa27esuXT4L89XUCi4DuP/Arj69B82Y/tvK5ZshqNbWtmdP3+jerS/EY/rMcUaGxtu37tu25Q91DY3pM8YmJMRTG8IZ5PSZo7NmLurSpReLXXCS+v2PnVMmzz535lZtV4+Nm1bs2+e3dMn6M6duaGvpbN22tsz93+m3EfZq3NhpmzbutrCwmjl7QkxsNMxns9kXLp6GfG5c78/j8aTrHz96GX5OnDDjYMC5os+TmZk5b/5UHW3dHdv2z561+OzZ41FRkWx2GafRvLy8WbMnvn37Gt6R/86DkP/ZcyaFh4cShH6CLLvWHB1qtG3rzWQyIaKdvHvcv39LJBLB4X7u/AkIMCyytLCCcrttG+/DR/YVbFB4s053d8/27Trb2ztoampyOBzYHMp8CNK58yfV1TXmzF5SrZoj/Js3ZxnUAq5eu1i4HSMrK6tnj/4NG3iZm1lQr96ieWt4XRaL1bxZa6FQ2KFDV0NDI3jCpk1bhoWFlL7nAoEAsj140Eh4khrVa/pOnVfPs1F09GfqtXhc3uhRk5ydaxdNqbZ2wQ0JNTQ0dLT/c2fCvx/dz8jMmDRxpoNDdagyQFzT09NIWZ4+exzy4f103/l1POrZ2NhNGD8dGilw/iII/QRZtr0dHZ2k07Y29tnZ2YmJCcnJSRBLz7r/b5q6udWFLEECqYe1arkW+2xQza7u6CRNFAQJ+tuKBvWbDa2tbL+uyecXfcjX4GcXgqiTEnz8GAYrSCvwUDtYvGiNdCkEm/ywyMgI2GdbW3vqoYmJKZxlytwqKCgQXtTdrS71EM5xUAcJDQ0mCJXFxsaGuq3192QZbyhspdM89YIrMUI5JhQKYGKq72jpHlC36UxOSaIeQlO52GeDDQ30DYvO0dDgU89W7Ibswqq+FIfLLfowv9SLDEKXHvzkcnnFLi1pD4slFAlhP4vO+eZh8VsJBdD4b9u+sXRObm6uvr4BQagsnz59KunwlmW8i2aPmtbW0qaGi+bNXQbN16IrGxuZJHyJL+XZIFQCQWbROfDwm8DLio6uHvnv/lcY1OShX6DoHOrcQQrr+d+snCXOoibgzULlYrf/f0b7oQwnCP0EWR5Ar18/l04HB7+D9jP0k0MnMFQ7U1KSoWFM/YNWK7SuS6kqU2pUrxUcEpTz72AyVASgW96prA7wirGytIG9ffXv/kNH1+SpI69evVjmht+fNaFRAPX8T58iqIfQQQjvnZrmFxbj0MFOPYT5SUmJ1DS8L9gKSmzpb4nD4Roa0ud+SahKyDLeiUlfYGwpOibq0aMH5y+c/LVFWy6XCx1m3t7dYT4MjEFf9IuXT6E/fNWaRWU+G3SJi8VZa9YtgYRAH/Ky5fOgiINuOSIHsJPQvXfo8O/Xrl2Cc8qGjStgNM7F1b2UTbiF4IzwITQYOhek82EkHLoJNm1e9S4o8OXLZytXL6RGB4GxsSlMw9gbrA9nqy1b12j/2y1Xt0596JhcsXIBbBIbF3Pj5pVRo/tDlyRB6CfIsnLesUNXOGrHjR+SnS1u1PAXGDSi5o8bM1VLU2vX7i1QWEF7snGjpiOGjy/z2SzMLdeu3g5D4j6j+kF/uKuLOwxN6RbWouVh9KjJDCbTb9dmkUhoZ+cAo3SwA6Vv0q/v0KPH9v/99/2DAWelMyHAixethcG/yVN8oPcbhq9h5I9aBBUWGCqDcfJOXZpD1H1GjIfmCdQUSMGwImv1qq07/TctXDwT6vampuaDBvnAICJB6Ccwim2UP7mSDK1C9xb65Id16dayR/d+gwf5EPRfw0b0hi7xyZNmETk7uCxs1Ap7lhreOkq1dO3adevWrVZWVt8vwm+MIURbqhLvN29ezp0/paSlBwPOffPpFIRoQGbxPnfmJlFg0DW97/eTJS2FATwiN3/sPU4QqgqqUnrD4JyBgVzGzBFSWNj2Roi2MN4I0RbGGyHawngjRFsYb4RoC+ONEG1hvBGiLYw3QrRVfLyZbILfTFA6mrrsfILQ/xX/fW++FjsjCS/Kr0yyBLliUR4bT8qoiOLjrW/GzRbnEqQ80pMkVtU1CEJFFB9vE2uuGocR+jKDICXx6GK8Z2t5XesCKakSL8bUbohpVHBm8D9lX6MbVa1cSf6l3VG/9jU2MOMQhIooree80yiz28cTTm/5pK2vxuOzCC3k5eUxmAwGoUMbVUNb7XNwJl+H5dVJ38yORxD6rzIGxlr0Ns7KzEuMFQvSJIQW9u3b98svv9hXq0aUH5vDcG1sqm+KhTYqXtnj3jxNpqWjOqGLtN+DDGzr1/DUIgjRHX6sBSHawngjRFsYb4RoC+ONEG1hvBGiLYw3QrSF8UaItjDeCNEWxhsh2sJ4I0RbGG+EaAvjjRBtYbwRoi2MN0K0hfFGiLYw3gjRFsYbIdrCeCNEWxhvhGgL440QbWG8EaItjDdCtMUkKkZDQ+PVq1cEIVoICQnhcrlqamrFLmXk56vWTWMzMzNXrVp1//79Xr169ezZ09TUlCCkhC5dunTy5MmsrCxfX19PT89i11G5eFMg5CcLVatWDXLepEkTgpAyiI6Opg7dFi1aQPlUu3btUlZW0XhLPXjwAH5THz58gN8U5FxTU5MgpJDu3r0Lx+qnT596FoJmZpmbqHq8KXFxcdQZ0cvLC0Lu7u5OEFIMaWlpcGSeOHGiVq1akOrGjRv/+LYY7/+4cuUK/CozMjKowpwgVHWePXsGqX7y5Al1NBoZGZFywngXIzQ0lCrMe/ToAb9ZR0dHglBlyc3NhVTD4aevrw+pbt26NakojHdpqJDz+XwIefv27QlC8vT+/Xs43s6fP08N69jZ2ZGfg/Eu28uXL+GXDmNpVJeGmZkZQUimLl68CMdYTk4OHGDdunUjMoLx/lFFx9Lgb/DLL78QhH5OVFQUdVC1bNkSDipXV1ciUxjvcqPG0kJCQqgalJaWFkGonO7cuQNH0efPn6kqobq6OpEDjHcFxcfHU/0fMJYGfx4PDw+CUFlSU1Op4trZ2RkOm0aNGhF5wnj/LGosLT09nSrMGQwGQeg7T58+hePkn3/+oUa5DA0NifxhvGUjLCyMKsy7d+8OfzwcS0MUiURCFdcGBgZwYLRq1YpUIoy3jJ06dQpyrqGhAX9LHEtTZdQo14ULF6hqna2tLal0GG+5ePXqFYT83r17VE0Mx9JUCjXKBeU2/PW7du1Kqg7GW44EAgH1aWF7e3v4Szdt2pQg+qJGueDP3bp1a/hzu7i4kKqG8a4Mf/31F/zhg4ODqVEQbW1tgmgERrkg1dHR0dTfl8fjEcWA8a48MJZG9bLAcAgcBHXq1CFImcEoF9Wf6urqCn/Qhg0bEgWD8a4CV69ehWMiLS2NOtkzmd9eEgv65IYMGdK3b1+CFBKMb8Ff8NmzZ9RfsHJGuSoA411lYCyNKsy7desGh0j16tWli6Ac4HK5CxYsqORxFFS6nJwc6k9mZGQEfzLF/+tgvKsejKXBEQMNNuhj79ChA8ypW7cug8HQ19ffvHlzzZo1CapqQUFB8De6dOkSVVxXyShXBWC8FcXr16+hIQedNHl5eWKxmJoJI2qHDh3CrrgqBAPX8HeBP0qVj3JVAMZbsQiFwiZNmkhb4/DXgUE1ONlEwfwAAAo/SURBVLwIqlyfP3+mus3atGkDtSpnZ2eihDDeiqV79+6RkZHfzIS6ur+/P0GV4tatW5DqmJgY6tNm0AlClBbGuyrB7/7tw7Tk+BxBqoSa8/zF82/+IlCSQzsc6uf2dvYEyRPUwAMDA/mafOg509YqvkHE12Wra7LM7dQtHBVlcLsUGO8qEx8pPrsj2s5F09hanc3G75kpB6YaM/GzSCzKY3NIi17lvrZhJcN4V43YiKxHl5NbDTAnmGvl9PR6EodDvDobEAWmcvcYUwR5eeTy77GYbaXm2dpAJMh/9ziDKDCMdxUIfppu4aCB2VZ2dq6a7/5OIwoM410FUhJyDK3kcm0tVJkMzblZolyiwK1bjHcVEKXnslgEKTsmi5GRLMmVKG6+2QQhRFMYb4RoC+ONEG1hvBGiLYw3QrSF8UaItjDeCNEWxhsh2sJ4I0RbGG+EaAvjjRBtYbwRoi38SgmSgWUr5k+cPILIx8JFM32nj/3x+UgKS2+k6Ly9u0tycqjpRYtnNWzYpF3bTt/MR8XCeCNFV8/z//fuCgkJgnh/Px8VCyvnSiAiIqxFS8+HD+8NHd5r7LjB1Mybt66OGTuofccm3Xu22bZ9fVZWFjX/9esXk6b4dOrSvIP3L1BhfvXqOTVfIpHs2+8/eGiPtu0bDxzc7dz5k9LnT0lJXrHqt56921GLTp8+WsrrXr16ER7CmkOG9fzzynnpk7BYrPsPbg8a0r1124bDffq8D35X5vvKycnZvWdbrz7t4V3ArgYGvqLmd+3e6uSpw7PmTGrTrlFmZqa0Eg47ExsXs3rNYnh35L+V8zdvXo4Y2RfWh726d//W+InD1m9YDvNhN2CrojszcFDXnX6bqOnU1BR44336dWzXwWvchKEvXj4l9IKltxJQU1ODn/sP7OrTe1CN6rVg+sGDO8uWz+vfb+j8+SuioiI3bFyelp46b85SkUg0d/6UX1u09Z06Lz8//+y547PnTjp29LK2lraf/+ZLl89MmTTb2cXt2bPH27avY7PZHTsU3HZjzbolnyM/Lpi3Ql/f4E3gSwiGsYlpE6/m37/u3Xs3YeWRPhM8POq9fv18zdol6uoazZsV3GorIT7uwoVTM6f/BtObtqxaueq3/X+cLP197fTbeOv2tcmTZpmbW545e2zm7Al7dh81N7OAHbtw8XTjRk0HD/QpejPd40cv9+7bYeKEGS1btiv6PHAKmDd/qoNDjTnbluRIcnbv3gq/k+qOTqW/el5e3qzZEzMFmbNmLjLQNzx3/sTsOZN2bj9gb+9A6ALjrQwYBZdlc3f3bN+uMzXj8NF9bm51IGYwbWlhNdJn4oqVC0aOmCASCQUCQetWHWxs7GDRhPHTmzdrzVHjQADg8B3Qf1jbtt7UJh8+vD98ZB8V7/HjfJlMJuQKpq2sbM6dO/H06SOI9/eve+LkIZjft09BSV6jes3k5KSkxC/UouSUpJ07Dujo6MJ09259161fBi+qqalZ0nuC/bx0+ezoUZNbNG8ND+F8JBIKo6M/w24wGAwelzd61KRvNtHW1oGfGhoaOoUTUn8/up+RmTFp4kxb24JLwUNc+/b3JmV5+uxxyIf3G9b7ebh7Ur8rmHP6zNHpvvMJXWC8lUatWq7UBBQ70AQdOmS0dJG7W134GR7+oV69RpDP5Svnd+7U09OzoaNDDXf3gkVQRYfKuWfd/zdW3dzqQrqEQiGkRZ2nDueLly+fpqWlwpNnZKRbWFh9/7qksOlb9HWLJtDK0obKNtDT1YefcK4pJd4fP4ZlZ2fXdPp6cx+oKSxetEa61Nm5NvlhkZERUOBT2QYmJqaGhmVfgTwoKBBelPrVkcLbRdR29QgNDSY0gvFWGnz+16hAMzs3Nxca0gcCdhddISk5ERrAWzbtOXJ0/6VLZ6BZCwf68KFj27TpKBQKYIWpvqMZjK/XZ6Wubw9FLofDgVoxPCEUX9ZWtvAM83/zLel1obXM4xV/EUie+v/nU69S+iX04SQCP7lcXulv9kcIRXCS4hed883D4rcSCuDtQCeCdA78EqB5QmgE4618oDkKhRVUgKmqtZSuXkGZqaurN3bMFPj38WP48RMHV65eaGNrT6Vl3txl9nb/aVgaG5lAIRYeHrp54+7atT2omWmpKWam5sW+LqDOFD9PR1ePFGaM/DSoyWdliYrOoc4d5N8TTVFZ4q99kPA7gVPbbv/DRZdKb95ID9hzrnzgEHR0dIqPj7W2tqX+mZlZsNhs6D+LiY2GXjdqNaisTps6F1b+GBFmb+8IFVHoIZduAu1YqEvD8S3OLrjZsPa/rdm3b19D73RJBS90X0GPmvTh1u3r4B+pEKjMw8ni1b/PBo2CyVNHQrd8mRt+v29Q6YB6/qdPEdTDz58/wTulpvmFxXhm5tebDcB86C2gpp2cnGErKLGlvxMOh2toaExoBOOtlKBzC4Z/oG8MDuUPocHQrzZp8gjorILu64WLZ0KhHRn5ERYFHNwD8YbGM7SBvb27Q30eeqrhFAAjQNNnjlu1ZhE8lUO16hBy6FKC4/6fp4+2bF0D48mfo/6fkKJ69ugP6/yxzw+Gmk6dPnr27PGaTi6kQmCXoMfu0OHfr127FBwStGHjCmjYu7i6l7IJtxCcEeAtQ1eCdD6MhEMPwqbNq94FBb58+QwqLNJeAGNjU5i+dv0SrA/db/DupCeyunXqQ98E/OpgEzij3bh5ZdTo/tABSWgEK+dKqekvv86ds/TI0X2QNKhkuri4bVzvz+fzoSNt1oyFx08ehPnQiraxsV+6eB10tsEm48ZM1dLU2rV7C8QYWpgw7DRi+HhSWJmfOWPhnj3bIAPVq9eEbucviQlLl82ZNn3M0iXrv3ndZk1bTpk8G04f0Lw3MTGDzupW/x2jKhfoNmcwmX67NkMnnJ2dw8rlmy3MLUvfpF/foUeP7f/77/sHA85KZ0KAFy9aC0N9k6f4wF7BgAIM5lGL4Mw1e9bi7TvWw1A5RN1nxPiEL/FQUyCFA/WrV23d6b8JTohQtzc1NR80yKdXzwGERvAWglXg5uEEfQueg7s2QfIxbERv6BKHEXUiZweXhY1aYc9SU9AbSmHpjRBtYbyRvLx583Lu/CklLT0YcO6bT6cgmcN4I3mBrul9v5f4uVTo5ydy88fe4wRhvJH8wFCcgYEhQVUH440QbWG8EaItjDdCtIXxRoi2MN4I0RbGGyHawngjRFsYb4RoC+ONEG1hvKuAuhZLlJlLkJLLzspjsRkK+3UxgpdzqBKGFtyMZLy9htJLTcg2s1MnCgzjXQWq19GMjRBmpkoIUmYvbyd5NNclCgwv51A1BGm5f+6La9DBSNeYQ5ASun0stqanlmOdclzRtfJhvKuMMD334p4YBoNhYqPOZClu+w0VpcZjxn8UEQaxc9Zw9VL076tjvKtYbERWYky2KAMr6sqBy2NqGaiZ2qhraCtBwxbjjRBt4cAYQrSF8UaItjDeCNEWxhsh2sJ4I0RbGG+EaOt/AAAA///RLsSzAAAABklEQVQDAPQTE2saiQ71AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3076e0cd0c4e"
      },
      "source": [
        "### Define the podcast topic\n",
        "\n",
        "This cell defines the topic of the podcast that the AI agent will create.\n",
        "\n",
        "The topic is assigned to the variable `PODCAST_TOPIC`. Feel free to modify this variable to explore different podcast topics!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "6627153c6715"
      },
      "outputs": [],
      "source": [
        "PODCAST_TOPIC = \"Explore the use of bio-inspired fluid dynamics in the design of underwater robots and vehicles\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1df657ce053"
      },
      "source": [
        "### Run the AI podcast agent\n",
        "\n",
        "This cell executes the compiled LangGraph workflow, running the AI podcast agent to generate the podcast script.\n",
        "\n",
        "The code performs these actions:\n",
        "\n",
        "- **Clean agent helper function:**  This function prepares the agent's output for printing by removing unnecessary characters and formatting\n",
        "- **Thread Configuration:**  A thread configuration is defined to ensure a unique history for this workflow execution\n",
        "- **Workflow Execution:** The `graph.stream()` method runs the workflow, iterating through each stage and updating the agent's state\n",
        "- **Output Display:** The code prints the results of each stage, including the agent's actions and generated output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "338377bc8c25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842
        },
        "outputId": "8f992edb-b8f3-474f-86cc-b4b7c2490f0e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "PermissionDenied",
          "evalue": "403 Permission denied on resource project None. [reason: \"CONSUMER_INVALID\"\ndomain: \"googleapis.com\"\nmetadata {\n  key: \"service\"\n  value: \"aiplatform.googleapis.com\"\n}\nmetadata {\n  key: \"containerInfo\"\n  value: \"None\"\n}\nmetadata {\n  key: \"consumer\"\n  value: \"projects/None\"\n}\n, locale: \"en-US\"\nmessage: \"Permission denied on resource project None.\"\n, links {\n  description: \"Google developers console\"\n  url: \"https://console.developers.google.com\"\n}\n]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/grpc/_interceptor.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    276\u001b[0m     ) -> Any:\n\u001b[0;32m--> 277\u001b[0;31m         response, ignored_call = self._with_call(\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/grpc/_interceptor.py\u001b[0m in \u001b[0;36m_with_call\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    331\u001b[0m         )\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/grpc/_channel.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;34m\"\"\"See grpc.Future.result.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/grpc/_interceptor.py\u001b[0m in \u001b[0;36mcontinuation\u001b[0;34m(new_details, request)\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m                 response, call = self._thunk(new_method).with_call(\n\u001b[0m\u001b[1;32m    316\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/grpc/_channel.py\u001b[0m in \u001b[0;36mwith_call\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1197\u001b[0m         )\n\u001b[0;32m-> 1198\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_end_unary_response_blocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_InactiveRpcError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pytype: disable=not-instantiable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.PERMISSION_DENIED\n\tdetails = \"Permission denied on resource project None.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:142.251.163.95:443 {created_time:\"2025-04-23T20:58:23.807619676+00:00\", grpc_status:7, grpc_message:\"Permission denied on resource project None.\"}\"\n>",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mPermissionDenied\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-632d9e80b057>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Run the LangGraph workflow, passing the initial state and thread configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m for state in graph.stream(\n\u001b[0m\u001b[1;32m     20\u001b[0m     {\n\u001b[1;32m     21\u001b[0m         \u001b[0;34m\"task\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPODCAST_TOPIC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[0m\n\u001b[1;32m   2431\u001b[0m                 \u001b[0;31m# with channel updates applied only at the transition between steps.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2432\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2433\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   2434\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2435\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-5378b13a0381>\u001b[0m in \u001b[0;36mpodcast_outline_node\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mHumanMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"task\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     ]\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"outline\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    366\u001b[0m         return cast(\n\u001b[1;32m    367\u001b[0m             \u001b[0;34m\"ChatGeneration\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    369\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    935\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    936\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m                 results.append(\n\u001b[0;32m--> 759\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    760\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1000\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m             result = self._generate(\n\u001b[0m\u001b[1;32m   1003\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_google_vertexai/chat_models.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_gemini_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_non_gemini\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m         return self._generate_gemini(\n\u001b[0m\u001b[1;32m   1375\u001b[0m             \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m             \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_google_vertexai/chat_models.py\u001b[0m in \u001b[0;36m_generate_gemini\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1608\u001b[0m     ) -> ChatResult:\n\u001b[1;32m   1609\u001b[0m         \u001b[0mrequest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_request_gemini\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1610\u001b[0;31m         response = _completion_with_retry(\n\u001b[0m\u001b[1;32m   1611\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_content\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1612\u001b[0m             \u001b[0mmax_retries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_google_vertexai/chat_models.py\u001b[0m in \u001b[0;36m_completion_with_retry\u001b[0;34m(generation_method, max_retries, run_manager, wait_exponential_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m         \u001b[0;32melse\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     )\n\u001b[0;32m--> 629\u001b[0;31m     return _completion_with_retry_inner(\n\u001b[0m\u001b[1;32m    630\u001b[0m         \u001b[0mgeneration_method\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mwrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mwrapped_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mretry_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mWrappedFn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0mretry_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetryCallState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mexc_check\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    418\u001b[0m                 \u001b[0mretry_exc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry_error_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_attempt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfailed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_attempt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: B902\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                     \u001b[0mretry_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_google_vertexai/chat_models.py\u001b[0m in \u001b[0;36m_completion_with_retry_inner\u001b[0;34m(generation_method, **kwargs)\u001b[0m\n\u001b[1;32m    620\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mretry_decorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_completion_with_retry_inner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneration_method\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgeneration_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m     params = (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/aiplatform_v1beta1/services/prediction_service/client.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   2394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2395\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2396\u001b[0;31m         response = rpc(\n\u001b[0m\u001b[1;32m   2397\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2398\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPermissionDenied\u001b[0m: 403 Permission denied on resource project None. [reason: \"CONSUMER_INVALID\"\ndomain: \"googleapis.com\"\nmetadata {\n  key: \"service\"\n  value: \"aiplatform.googleapis.com\"\n}\nmetadata {\n  key: \"containerInfo\"\n  value: \"None\"\n}\nmetadata {\n  key: \"consumer\"\n  value: \"projects/None\"\n}\n, locale: \"en-US\"\nmessage: \"Permission denied on resource project None.\"\n, links {\n  description: \"Google developers console\"\n  url: \"https://console.developers.google.com\"\n}\n]"
          ]
        }
      ],
      "source": [
        "# Function to clean and format agent output for display\n",
        "\n",
        "\n",
        "def clean_agent_result(data):\n",
        "    agent_result = str(data)\n",
        "    agent_result = re.sub(\n",
        "        r\"[^\\x00-\\x7F]+\", \" \", agent_result\n",
        "    )  # Remove non-ASCII characters\n",
        "    agent_result = re.sub(r\"\\\\\\\\n\", \"\\n\", agent_result)  # Replace escaped newlines\n",
        "    agent_result = re.sub(r\"\\\\n\", \"\", agent_result)  # Replace newlines\n",
        "    agent_result = re.sub(r\"\\\\'\", \"'\", agent_result)  # Replace escaped single quotes\n",
        "    return agent_result\n",
        "\n",
        "\n",
        "# Thread ID for unique history in workflow execution\n",
        "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "# Run the LangGraph workflow, passing the initial state and thread configuration\n",
        "for state in graph.stream(\n",
        "    {\n",
        "        \"task\": PODCAST_TOPIC,\n",
        "        \"revision_number\": 1,  # Current revision number\n",
        "        \"max_revisions\": 2,  # Maximum number of revisions allowed\n",
        "        \"search_count\": 0,  # Current search number\n",
        "        \"max_searches\": 3,  # Maximum number of searches allowed per revision\n",
        "        \"content\": [],\n",
        "        \"queries\": [],\n",
        "        \"tool_calls\": [],\n",
        "    },\n",
        "    thread,\n",
        "):\n",
        "    # Print a snippet of the results of each workflow stage\n",
        "    for k, v in state.items():\n",
        "        print(f\"Agent Node: {k}\\n\")\n",
        "        print(\"Agent Result:\")\n",
        "        print(clean_agent_result(v)[:1000])\n",
        "    print(\"\\n====================\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd8d951762fb"
      },
      "source": [
        "### Parse and display the final podcast script\n",
        "\n",
        "This section extracts and prepares the final podcast script generated by the AI agent.\n",
        "\n",
        "It displays the script for review, where each string in the list will be narrated by a different text-to-speech voice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "599397cab03c"
      },
      "outputs": [],
      "source": [
        "podcast_script = state[\"generate_script\"][\"draft\"]\n",
        "parsed_script = [\n",
        "    text for text in (line.strip() for line in podcast_script.splitlines()) if text\n",
        "]\n",
        "parsed_script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e075edfe820d"
      },
      "source": [
        "### Generate audio for the podcast\n",
        "\n",
        "This cell generates audio for each line of the parsed podcast script using Google Cloud's Text-to-Speech API.\n",
        "\n",
        "It creates separate audio files for each line, alternating between two different voices to simulate a conversation between two podcast hosts.\n",
        "\n",
        "The code:\n",
        "\n",
        "1. **Initializes the Text-to-Speech Client:**  Sets up the connection to the API.\n",
        "2. **Defines Audio Configuration:** Specifies the desired output audio format (MP3).\n",
        "3. **Iterates through Script Lines:** Generates audio for each line, alternating voices.\n",
        "4. **Saves Audio Files:**  Writes the generated audio to separate MP3 files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c74e3badfd35"
      },
      "outputs": [],
      "source": [
        "# Instantiates a client\n",
        "client = texttospeech.TextToSpeechClient()\n",
        "\n",
        "# Select the type of audio file you want returned\n",
        "audio_config = texttospeech.AudioConfig(audio_encoding=texttospeech.AudioEncoding.MP3)\n",
        "\n",
        "audio_files = []\n",
        "for count, line in enumerate(parsed_script):\n",
        "    # Set the text input to be synthesized\n",
        "    synthesis_input = texttospeech.SynthesisInput(text=line)\n",
        "\n",
        "    # Choose the voice for the current line, alternating between hosts\n",
        "    if count % 2 == 0:\n",
        "        voice_name = \"en-US-Chirp3-HD-Aoede\"\n",
        "    elif count % 2 == 1:\n",
        "        voice_name = \"en-US-Chirp3-HD-Puck\"\n",
        "\n",
        "    # Configure voice parameters: language and voice name\n",
        "    voice = texttospeech.VoiceSelectionParams(\n",
        "        language_code=\"en-US\",\n",
        "        name=voice_name,\n",
        "    )\n",
        "\n",
        "    # Generate audio using the Text-to-Speech API\n",
        "    response = client.synthesize_speech(\n",
        "        input=synthesis_input, voice=voice, audio_config=audio_config\n",
        "    )\n",
        "\n",
        "    # Save the generated audio to an MP3 file\n",
        "    filename = f\"part-{str(count)}.mp3\"\n",
        "    audio_files.append(filename)\n",
        "    with open(filename, \"wb\") as out:\n",
        "        out.write(response.audio_content)\n",
        "        print(f\"Audio content written to file {filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea674bbf2628"
      },
      "source": [
        "### Combine audio files and generate final podcast\n",
        "\n",
        "This cell combines the individual audio files generated in the previous step into a single podcast file.\n",
        "\n",
        "It also adds brief silences between each line for better listening experience.\n",
        "\n",
        "The final podcast is saved as `gemini-podcast.mp3`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9a4e93adc415"
      },
      "outputs": [],
      "source": [
        "# Initialize audio segment\n",
        "full_audio = AudioSegment.silent(duration=200)\n",
        "\n",
        "# Concatenate audio segments with silence in between\n",
        "for file in audio_files:\n",
        "    sound = AudioSegment.from_mp3(file)\n",
        "    silence = AudioSegment.silent(duration=200)\n",
        "    full_audio += sound + silence\n",
        "    os.remove(file)  # Remove the individual part files after combining\n",
        "\n",
        "# Save the final audio output to a file\n",
        "podcast_filename = \"gemini-podcast.mp3\"\n",
        "full_audio.export(podcast_filename)\n",
        "print(f\"Podcast content written to file {podcast_filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2be9b47ae8a3"
      },
      "source": [
        "### Listen to your AI-generated podcast!\n",
        "\n",
        "This cell plays the final podcast generated by the AI agent.\n",
        "\n",
        "The `Audio` object from `IPython.display` is used to embed the audio player directly into the notebook. The podcast will start playing automatically.\n",
        "\n",
        "Enjoy your AI-created podcast!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f50dcfe1651"
      },
      "outputs": [],
      "source": [
        "Audio(filename=podcast_filename, rate=32000, autoplay=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df34f0351197"
      },
      "source": [
        "## Conclusion: Building AI-powered podcast agents\n",
        "\n",
        "This notebook showcases the exciting potential of using AI to automate the podcast creation process. By combining the power of the Gemini API with the flexibility of LangGraph, you built an intelligent agent capable of:\n",
        "\n",
        "- **Generating Podcast Outlines:** Structuring the flow and content of the podcast.\n",
        "- **Conducting Research:**  Gathering information from various sources like arXiv, PubMed, and Wikipedia.\n",
        "- **Writing Engaging Scripts:** Crafting podcast scripts with natural-sounding dialogue, citations, and a conversational style.\n",
        "- **Critiquing and Revising:** Providing feedback on the script and iteratively refining it.\n",
        "- **Generating Audio:**  Using text-to-speech technology to create the final podcast audio.\n",
        "\n",
        "This is just a starting point! You can customize this workflow further by:\n",
        "\n",
        "- **Adding New Research Tools:**  Integrate additional sources of information relevant to your podcast topics.\n",
        "- **Experimenting with Prompts:** Refine the prompts to guide the AI agent towards your desired style and content.\n",
        "- **Exploring Different Voices:**  Use a wider range of voices for the podcast hosts to create unique and engaging listening experiences.\n",
        "\n",
        "The possibilities are endless! As AI technology continues to advance, you can expect even more creative and innovative applications in podcasting and other content creation domains.\n",
        "\n",
        "You can learn more about [LangGraph](https://langchain-ai.github.io/langgraph/), the [Gemini API in Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models), or the [chat model provider for Vertex AI in LangChain](https://python.langchain.com/docs/integrations/chat/google_vertex_ai_palm/) in their respective documentation pages."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "langgraph_gemini_podcast.ipynb",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}